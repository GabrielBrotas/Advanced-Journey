{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Advanced Journey  <p> Contains my learning related to Architecture, DevOps and Backend tools.</p>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li>Types of Architecture</li> <li>Sustainability</li> <li>Architectural Features</li> <li>Design Quality Software</li> </ul>"},{"location":"#systems-communication","title":"Systems Communication","text":"<ul> <li>Notes</li> <li>REST</li> <li>gRPC</li> <li>GraphQL</li> </ul>"},{"location":"#solid","title":"S.O.L.I.D","text":"<ul> <li>Notes</li> <li>S \u21d2 SRC \u21d2 Single Responsibility Principle</li> <li>O \u21d2 OCP \u21d2 Open-closed Principle</li> <li>L \u21d2 LSP \u21d2 Liskov Substitution Principle</li> <li>I \u21d2 ISP \u21d2 Interface Segregation Principle</li> <li>D \u21d2 DIP \u21d2 Dependency Inversion Principle</li> </ul>"},{"location":"#domain-driven-design","title":"Domain Driven Design","text":"<ul> <li>Notes</li> <li>Project</li> </ul>"},{"location":"#microservices","title":"Microservices","text":"<ul> <li>Notes</li> <li>Project</li> </ul>"},{"location":"#kafka","title":"Kafka","text":"<ul> <li>Introduction</li> <li>Topics</li> <li>Producers and Consumers</li> <li>Kafka Connect</li> <li>Avro</li> </ul>"},{"location":"#rabbitmq","title":"RabbitMQ","text":"<ul> <li>Notes</li> </ul>"},{"location":"#kubernetes","title":"Kubernetes","text":"<ul> <li>Notes</li> <li>Resources</li> <li>Project with K8s</li> </ul>"},{"location":"#observability","title":"Observability","text":"<ul> <li>Notes</li> <li>Service Level</li> <li>Elastic Stack</li> <li>Open Telemetry</li> </ul>"},{"location":"#computer-science","title":"Computer Science","text":"<ul> <li>Program, Process, Thread, Core...</li> <li>Object-Oriented Programming vs Functional Programming</li> <li>HTTPS, TLS and mTLS</li> <li>Reactive Programming vs Reactive Systems</li> </ul>"},{"location":"#gitops","title":"GitOps","text":"<ul> <li>Notes</li> </ul>"},{"location":"domain-driven-design/","title":"Domain Driven Design","text":"<p>Defines how to model software in general.</p> <p>It is a way of developing software with a focus on the heart of the application - what we call the domain - with the objective of understanding its rules, processes and complexities, thus separating them from other complex points that are normally added during the development process.</p> <p>DDD is about modeling ubiquitous (universal) language within a bounded context.</p>"},{"location":"domain-driven-design/#complex-software","title":"Complex software","text":"<ul> <li>DDD is / should be applied for cases of complex software projects;</li> <li>Large projects have many areas, many business rules, many people with different views in different contexts;</li> <li>There is no way not to use advanced techniques in highly complex projects;</li> <li>Much of the complexity of this type of software does not come from technology, but from communication, context separation, understanding the business from different angles;</li> <li>People (Product Manager, Owner, Developer, Experts, Architects, ...)</li> </ul> <p>DDD should be applied to large projects, which have several areas, several business areas, and people who are in different contexts.</p> <p>DDD was created to provide more clarity in the project, to understand that the software is not just another one. Our role as a developer is to understand this and know how to model for each type of system and context.</p> <p>Domain-Driven Design is, above all,\u00a0communication. In DDD modeling and implementation go hand in hand.</p> <p>Domain experts (users, analysts and other experts in the field), along with developers and architects work hand in hand with a common goal:\u00a0build domain-driven software to meet customer needs.</p> <p>To do this, in the first place,\u00a0it is necessary that everyone use a common language\u00a0and that there is no translation in the communication between team members.</p>"},{"location":"domain-driven-design/#how-can-ddd-help","title":"How can DDD help?","text":"<ul> <li> <p>Understand in depth the domain (domain) and subdomains of the application. Domain is something that is connected with the main function of the application that we are going to develop, it is the heart of the software that we are going to develop; Subdomain is the separation of the domain into pieces, which make the pieces fit together to create the core of the application. It is important to understand the limit of each subdomain.</p> </li> <li> <p>Have a universal language (ubiquitous language) among all involved. Use jargon from the area that is used in that context.</p> <p>Ex: the customer of a B2B company are companies, but the customer of a bakery is people, so the entity 'customer' will not always be the same thing, it all depends on the context.</p> <p>If the company calls invoice ABC, in our application we should also call it that because it will facilitate communication between Solution Architect - Company, keeping everyone in the same context</p> </li> <li> <p>Create the strategic design using Bounded Contexts. Understand the context / boundaries of each subdomain</p> </li> <li> <p>Create the tactical design to be able to map and aggregate the application's entities and value objects, as well as domain events.</p> </li> <li> <p>Clarity of what is business complexity and technical complexity.</p> </li> </ul>"},{"location":"domain-driven-design/#domain-vs-subdomain","title":"Domain vs Subdomain","text":"<p>Domain is the problem as a whole, when we explore this domain/problem further, we begin to identify some points that can be treated in isolation, and each point has a degree of importance within the main domain.</p> <p>Example: Within Netflix, the main core (core domain) is the streaming service, listing the catalog and playing the movies, if you don't have this, it makes no sense for the application to exist</p> <p>Core Domain:</p> <ul> <li>Heart of the business;</li> <li>Competitive advantage of the company;</li> </ul> <p>Support subdomain:</p> <ul> <li> <p>Support the Domain on a day-to-day basis;</p> <p>For example: within an ecommerce, the products and checkout are the core domain, but a warehouse (distribution center) is needed, which works as the support subdomain</p> </li> <li> <p>Makes domain operation possible;</p> </li> </ul> <p>Generic subdomain</p> <ul> <li>Supports the subdomain and core domain but does not generate a competitive advantage for the company;</li> <li>Are easily replaceable;</li> <li>Auxiliary software;</li> </ul>"},{"location":"domain-driven-design/#problem-space-vs-solution-space","title":"Problem Space vs Solution Space","text":"<p>DDD \u2192 Understand the problem and how you model the problem so you can solve it.</p> <p>With that, we end up having a problem space and a solution space</p> <pre><code>Problem                             Solution\n-----------------------      ------------------------------\n(1) Domain overview      =&gt;   (3) Analysis and modeling of the \n                                  domain and its complexities.\n\n        \u2b07\ufe0f                             \u2b07\ufe0f\n        \u2b07\ufe0f                             \u2b07\ufe0f\n\n(2) Subdomains           =&gt;   (4) Delimited contexts\n</code></pre> <p>When we have an overview of the domain and its complexities, we begin to understand and separate the subdomains, still in the problem space.</p> <p>Solution \u21d2 How can I understand this problem and organize it in a way that I can solve all this, take each subdomain and delimit contexts, each generated context ends up becoming a sub product;</p> <p>This way we can have the domain modeled and the contexts delimited so that we can understand what we have to do and what is the priority.</p> <p>Subdomains with delimited contexts \u21d2 specific problem we have to solve.</p> <p>Bounded Contexts</p> <p>When we have a subdomain we can delimit them to create a bounded context.</p> <p>A bounded context is an explicit division within a domain model. A border/boundary of a domain we are modeling.</p> <p>One of the ways we manage to bring this delimitation is through ubiquitous language, we can understand where we are through the language that is being spoken in that context.</p> <p>When everyone is speaking the same language we realize that we are in the same context, when that language starts to change we begin to identify that we are going to another context.</p> <p>\u201cContext is King\u201d</p> <p>The context will always determine in which area of the company we are working, what type of problem we are trying to solve, and also the language can be the same from one place to another but have different meanings.</p> <p>Examples:</p> <ul> <li>Ticket sales \u21d2 Ticket // ticket purchase</li> <li>Customer support \u21d2 Ticket // support request identifier</li> </ul> <p>When we have two words that are the same but that represent different things, we are in a different context.</p> <p>If we are in a monolithic system, we will have to create different entities for each one according to its context.</p> <ul> <li>Finance \u21d2 eNFS</li> <li>Inventory \u21d2 XPTO (name for invoice)</li> </ul> <p>When we have two different words that means the same thing we are probably in a different context.</p> <p>Cross elements</p> <p>Often, despite being in different contexts, these models end up talking between entities / transversal elements.</p> <p>Example:</p> <ul> <li>Ticket sales \u21d2 Customer</li> <li>Customer support \u21d2 Customer</li> </ul> <p>Despite being different context is the same entity, however, despite being the same entity they will have different information.</p> <ul> <li>Ticket sales \u21d2 Customer \u2192 Event, Ticket, Location, Seller...</li> <li>Customer support \u21d2 Customer \u2192 Ticket, Question, Department, Responsible,...</li> </ul> <p>For each context, an entity will have to be customized for it, as having a class that wants to handle everything ends up being unfeasible and difficult to maintain</p>"},{"location":"domain-driven-design/#entities","title":"Entities","text":"<p>Entities are unique, they must have unique IDs, each one is different from the other;</p> <p>Entities carry attributes that can change over time (removing or adding);</p> <p>Anemic entities</p> <pre><code>class Customer {\n    _id: string;\n    _name: string;\n    _address: string\n\n    constructor(id: string, name: string, address: string) {\n        this._id = id;\n        this._name = name;\n        this._address = address\n    }\n\n    get id(): string {\n        return this._id\n    }\n\n    get name(): string {\n        return this._name\n    }\n\n    get address(): string {\n        return this._address\n    }\n\n    set name(name: string) {\n        this._name = name\n    } \n\n    set address(address: string) {\n        this._address = address\n    }\n}\n</code></pre> <p>Anemic entities are the type that only carry data and change the name of properties. And we usually create these entities to be manipulated by the ORM, these are entities that don't have a significant value in themselves.</p> <p>Rich entities are entities that have unique value, data can be changed, have behavior and carry business rules, and it is in these business rules that the heart of the business lives.</p> <p>Now, instead of entities just loading data, they will define how the entity should behave (business rules, self-validation, etc...), because if the client says that the XPTO entity now behaves differently, that's it entity that we must change.</p> <p>The first thing we must do when we think of an Entity is to think about what kind of behavior it will carry.</p> <p>Ex:</p> <p>This type of change doesn't have any expressiveness, it's just there for the sake of it, it's a method of changing an attribute</p> <pre><code>    set name(name: string) {\n        this._name = name\n    } \n</code></pre> <p>After:</p> <p>It means that this Entity, its business rule, allows a user to change its name, the entity needs this method</p> <pre><code>changeName(name: string) {\n        this._name = name\n    } \n</code></pre> <p>Consistency in the first place: an Entity will always have to represent the correct and current state of that element, that is, if in the database the entity has (_id, name, address and status) and all fields are mandatory, ALWAYS ALWAYS all data from that entity must have these attributes, if by any chance a piece of data does not have an address, it means that it is violating the DDD and the rich domain rules.</p> <p>Whenever we consult this entity, the data has to be consistent, since it is not consistent, we cannot validate business rules. When we talk about DDD we must be able to trust 100% of the time in the current state of the object.</p> <p>Principle of self-validation: An Entity, by default, it must always self-validate</p> <p>Problem:</p> <p>If we leave the responsibility to another system resource, the data may be inconsistent.</p> <pre><code>class Customer {\n    _id: string;\n    _name: string;\n\n    constructor(id: string, name: string) {\n        this._id = id;\n        this._name = name;\n        this.isValid()\n    }\n\n    get id(): string {\n        return this._id\n    }\n\n    get name(): string {\n        return this._name\n    }\n\n    changeName(name: string) {\n        this._name = name\n        this.isValid()\n    }\n\n    isValid() {\n        if(this.name.length &lt; 5) {\n            throw ValidationError(\"name length must the greater than 5.\")\n        }\n    }\n}\n\nconst newCustomer = new Customer(\"1\", \"\")\n</code></pre> <p>Architecture:</p> <pre><code>src/...\n\n    // Business complexity / business rules\n    // A single way to run, the way the customer is asking.\n    /domain/... \n\n    // Accidental Complexity / Conversation with the Outside World / Storing Data...\n    // n ways to solve, Excel, Sass, DB, Cloud,...\n    /infra/... &lt;- \n\n</code></pre>"},{"location":"domain-driven-design/#object-values","title":"Object Values","text":"<p>\u201cWhen you only care about the attributes of a model element, classify it as a Value Object\u201d</p> <p>A Value Object is immutable, it doesn't change it is replaced by another VO.</p> <p>EX:</p> <pre><code>class Address {\n    _street: string;\n    _number: number;\n    _zip: string;\n    _city: string;\n\n    constructor(street: string, number: number, zip: string, city: string) {\n        this._street = street;\n        this._number = number;\n        this._zip = zip;\n        this._city = city\n    }\n}\n\nclass Customer {\n    _id: string;\n    _name: string;\n    _address!: Address;\n\n    constructor(id: string, name: string) {\n        this._id = id;\n        this._name = name;\n    }\n\n    get id(): string {\n        return this._id\n    }\n\n    get name(): string {\n        return this._name\n    }\n\n    get address(): Address {\n        return this._address\n    }\n\n    set name(name: string) {\n        this._name = name\n    } \n\n    set address(address: Address) {\n        this._address = address\n    }\n}\n</code></pre>"},{"location":"domain-driven-design/#domain-services","title":"Domain Services","text":"<p>A domain service is a stateless operation that performs a domain-specific task. Often the best indication that you should create a Service in the domain model is when the operation you need to perform doesn't seem to fit as a method on an Aggregate or a Value Object.</p> <p>When a significant process or transformation in the domain is not the natural responsibility of an ENTITY or Value Object, add an operation to the model as a stand-alone interface declared as a SERVICE. Set the interface in based on the domain model language and make sure the operation name is part of the UBIQUITOUS LANGUAGE. Make the SERVICE stateless.</p> <p>Domain Service is stateless;</p> <p>Any alteration / transformation / transaction,..., anything that affects the domain and that we cannot perform in the Entity class itself, because we need access to another Entity, OV, or something external is because we need to use a Domain Service .</p> <p>These services run on the Domain layer, where the business rules take place.</p> <p>When to create domain service?</p> <ul> <li>Can an entity perform an action that will affect all entities?</li> <li>How to perform a batch operation?</li> <li>How to calculate something whose information is contained in more than one entity?</li> </ul> <p>Careful:</p> <ul> <li>When there are many Domain Services in your project, MAYBE this could indicate that your aggregates are anemic. (only get and set)</li> <li>Domain Services are Stateless (do not keep state)</li> </ul>"},{"location":"domain-driven-design/#repositories","title":"Repositories","text":"<p>A repository commonly refers to a place of storage, generally considered a place of security or preservation of the items stored there. When you store something in a repository and then go back to retrieve it, you expect it to be in the same state it was in when you put it there. At some point, you can choose to remove the stored item from the repository.</p> <p>These objects, similar to collections, are about persistence. Every Persistent Aggregate type will have a Repository. Generally speaking, there is a 1-1 relationship between an Aggregate type and a Repository.</p>"},{"location":"domain-driven-design/#domain-events","title":"Domain Events","text":"<p>Use a domain event to capture an instance of something that happened in the domain.</p> <p>The essence of a domain event is that you use it to capture things that might trigger a change in the state of the application you are developing. These event objects are processed to cause system changes and stored to provide an AuditLog.</p> <p>Perform an operation based on an event or store a log;</p> <p>Every event must be represented in an action performed in the past;</p> <p>Ex:</p> <ul> <li>UserCreated;</li> <li>OrderPlaced;</li> <li>EmailSent;</li> </ul> <p>When to use?</p> <p>Normally a Domain Event should be used when we want to notify other Bounded Contexts of a state change.</p> <p>Communication with external contexts;</p> <p>Components</p> <ul> <li>Event \u2192 contains the message along with the timestamp of when it occurred;</li> <li>Handler \u2192 concrete implementation of the event that is triggered, performs processing when an event is called, an event can have multiple handlers. ex: send email, call external api, send push notification,...</li> <li>Event Dispatcher \u2192 Responsible for storing and executing the handlers of an event when it is triggered; logs all events and their handlers.</li> </ul> <p>Flow:</p> <ul> <li>Create an \u201cEvent Dispatcher\u201d</li> <li>Create an \u201cEvent\u201d</li> <li>Create a \u201cHandler\u201d for the \u201cEvent\u201d</li> <li>Registers the Event, along with the Handler in the \u201cEvent Dispatcher\u201d</li> </ul> <p>Now to trigger an event, just execute the \u201cnotify\u201d method of the \u201cEvent Dispatcher\u201d. At that moment, all Handlers registered in the event will be executed.</p>"},{"location":"domain-driven-design/#modules","title":"Modules","text":"<p>In a DDD context, Modules in your model serve as named containers for classes of domain objects that are highly cohesive with each other. The goal should be loose coupling between classes that are in different modules. Since the Modules used in DDD are not anemic or generic storage bins, it is also important to properly name the Modules.</p> <ul> <li>Represents the application represented by the domain;</li> <li>The modules must respect the universal (ubiquitous) language;</li> <li>Low coupling;</li> <li>One or more aggregates must be together only if they make sense;</li> <li>Organized by domain / subdomain and not by type of objects</li> <li>They must respect the same division when they are in different layers (infra, domain, ...) it is easier to maintain a correlation;</li> </ul>"},{"location":"domain-driven-design/#factories","title":"Factories","text":"<p>Shift the responsibility for creating instances of complex objects and Aggregates to a separate object, which may not have responsibility in the domain model but is still part of the domain design. Provide an interface that encapsulates all complex creation and doesn't require the client to reference the concrete classes of the objects being instantiated. Create entire Aggregates at once, enforcing their invariants.</p> <p>Every time we need to create a complex object, we can delegate this to the factory.</p> <pre><code>Client =&gt; (Input, Specifies what it wants) =&gt;\nFactory =&gt; (Makes object that satisfies client and internal rules) =&gt;\nObject =&gt; Output\n</code></pre>"},{"location":"kubernetes/","title":"Kubernetes","text":"<p>Notes</p> <ul> <li>K8S is made available through a set of APIs;</li> <li>We normally access the API using the CLI: kubectl;</li> <li>Everything is state based. You configure the state of each object;</li> <li>Kubernetes Master: Control the entire process of what the others node are going to do;<ul> <li>Kube-apiserver</li> <li>Kube-controller-manager</li> <li>Kube-scheduler</li> </ul> </li> <li>Other Nodes:<ul> <li>Kubelet</li> <li>Kubeproxy</li> </ul> </li> </ul>"},{"location":"kubernetes/#structure","title":"Structure","text":"<p>Master(Control Plane) \u2192 Manage the entire process; it must be high available;</p> <ul> <li>Etcd \u2192 Database, key value store for critical info, it stores the current state of the cluster, define which container is going to each node, what time they are getting loaded, etc;</li> <li>Scheduler \u2192 Decide which worker node will be best to deploy the next pods, it takes multiple factors in the scheduleing decision, such as resource requirements of the container, policy, data locality's, internal workload,\u2026;</li> <li>Controller Manager \u2192 Ensures proper states of cluster components, configuration defined through a manifest file, ex: Desired State: 3 nodes, Current State: 2, it works to match the state requirements;</li> <li>API \u2192 Exposes kubernetes api, how we communicate with kubernetes master;</li> </ul> <p>Node(Data Plane) \u2192 Host containers, ex: a ec2 can work as a node,</p> <ul> <li> <p>Kubelet \u2192 Agent that runs in each node in the cluster, it makes sure the containers are running ok, verify the status of the node and containers to Control Plane, and, is how it communicate with the master node.</p> </li> <li> <p>Kube Proxy \u2192 How applications communicate with each other, it maintains network rules, this rules allows network communication to your conntainer from inside or outside of your cluster.</p> </li> </ul>"},{"location":"kubernetes/#components","title":"Components","text":"<p>Cluster: Set of machines</p> <p>Pods: Unit that contains the provisioned containers</p> <p>Pod represents the process running on the cluster. Normally a pod represents a container but we can have more than one container running in the same pod (even though is not so commom)</p> <p>ReplicaSet: it makes sure the desired amount of pods will be running, if a pod goes down it will create another one automatically</p> <p>Ex: </p> <pre><code>B = Backend \u21d2 3 replicas\nF = Frontend \u21d2 2 replicas\n</code></pre> <p>If we ask to add another Frontend Pod in the cluster and it has no more resources, the task will be pending until it has more computational resources or we create another node (cluster) to place this task on the next available node.</p> <p>k8s is monitoring the health of clusters and pods to be able to recreate if they are no longer available</p> <p>Deployment: Wrapper around replica set, it has the objective of provisioning the pods, it will define how many replicas we want from each Pod, but also it helps on the upgrade container version, scale, undo,\u2026</p> <p>Deployment will restore replicaset with running pods</p> <p></p> <pre><code>replicas: 3\n...\nminReadySeconds:\nstrategy:\n    # when update the container image to a new version\n    rollingUpdate:\n        maxSurge: 1 # it will create a new pod with the new image version and when available it will remove the older version\n        maxUnavailable: 0 # the update has to be done in such a way that there is at lest three pods running\n    type: RollingUpdate\n\n# 3 replicas + 1 maxSurge, a total of 4 pods at time\n</code></pre> <p>Service</p> <p>Expose an application running on a set of pods as a network service.</p> <p>Instead of a pod communicate with another using IP address it can use a service that distribute traffic to the pods.</p> <p>Is a service discovery tool that also works as load balancer.</p> <p>If a pod/node goes down this service will redirect the traffic for the new pods as well. it keeps track of new pods/nodes</p> <p>It works with label selector.</p> <p>Different types of service:</p> <ul> <li>ClusterIP Default type of service, only accessible from within cluster, if you are outside the cluster and you want to access the service you can't do it.</li> <li>LoadBalancer Cloud specific implementation, accessible from outside cluster, has dns name, ssl termination, WAF integration, Access Logs, Health Check, etc. when we run a 'kubectl get services' in a cloud provider the LoadBalancer type will generate an External IP to us so we can use this IP to access our service</li> <li>Nodeport Accessible from outside cluster, creates cluster wide pord. NodePort[30000-32767]  ex: 10.16.10.01:32000, that way you can access your application</li> </ul> <p>Namespaces</p> <p>Isolated environment, we can group resources separately like a database, app1, app2, ...</p>"},{"location":"kubernetes/#eks","title":"EKS","text":"<p>AWS manages the Control Plane(Master Node)</p> <ul> <li>AWS maintains high availability - Multiple EC2s in Multiple AZs</li> <li>Detects and replaces unhealthy control plane instances</li> <li>Scales control plane</li> <li>Maintain etcd</li> <li>Provides automated version upgrade and patching</li> <li>supports native and upstream kubernetes</li> </ul> <p>EKS Data Plane</p> <ul> <li>Amazon EC2 - Self Managed Node Groups<ul> <li>You maintain worker EC2s</li> <li>You orchestrate version upgrade, security patching, AMI Rehydration, keeping pods up during upgrade;</li> <li>Can use custom AMI</li> </ul> </li> <li>Amazon EC2 - Managed Node Groups<ul> <li>AWS manages worker EC2s</li> <li>AWS provides AMI with security patches, version upgrade,...</li> <li>AWS manages pod disruption during upgrade</li> <li>Doesn't work with custom AMI</li> </ul> </li> <li>AWS Fargate<ul> <li>No worker EC2 whatsoever !</li> <li>You define and deploy pods</li> <li>Container + Serverless</li> </ul> </li> </ul>"},{"location":"microservices/","title":"Microservices","text":""},{"location":"microservices/#differences-between-microservice-and-monolith","title":"Differences between Microservice and Monolith","text":"<p>Microservices are ordinary applications, there isn't any difference from any other application.</p> <p>The main difference are the goals, microservices has a well defined goal/domain and a monolith has all the ecosystem/responsability on the same system.</p> <p>Microservices are part of an ecosystem, it isn't an isolated application, usually are part of a bigger context.</p> <p>They are independent and communicate all the time directly or indirectly. </p> <p>Another big difference is the deployment process, microservices has less risk because the process is more independent.</p> <p>Another difference is the team organization, we can have one team per microservice</p>"},{"location":"microservices/#when-to-use","title":"When to use","text":"<p>Monolith</p> <ul> <li>Start of a project</li> <li>POC - Proof of Concept</li> <li>When you don't understand the domains</li> <li>Ensure technology governance</li> <li>Easier to understand the code flow</li> <li>Everything on the same spot</li> </ul> <p>Microservice</p> <ul> <li>Scale teams</li> <li>Well defined contexts / business rules/area</li> <li>Maturity in the delivery process</li> <li>Technical Maturity</li> <li>When you have the necessity of scale just one part of the system</li> <li>When you need different technologies</li> </ul>"},{"location":"microservices/#migration","title":"Migration","text":"<ul> <li>Context separation;</li> <li>Avoid excess granularity;</li> <li>Verify dependencies to avoid distributed monolith;</li> <li>Migrate databases;</li> <li>Think in events;</li> <li>Eventual consistency;</li> <li>CI/CD/Tests/Environments;</li> <li>Start from the edges, parts of the system that doesn't affect the main domain;</li> </ul>"},{"location":"microservices/#resiliency","title":"Resiliency","text":"<ul> <li>Health Check</li> <li>Rate Limiting</li> <li>Circuit breaker</li> <li>API Gateway</li> <li>Service Mesh</li> <li>Retry</li> <li> <p>Transactional Outbox</p> <p>Temporary table, is a local queue;</p> <p>First we save the message on this table and then we send the message to kafka, if the message was sent successfully we delete the message from the Outbox table, but, if some error happen to send the message to kafka we know that it'll be secure in our db</p> </li> <li> <p>Fallback policies</p> </li> <li>Observability</li> <li>Idempotency - deal with duplicated messages</li> </ul> <p>Complex Situations:</p> <ul> <li>What if the message broker goes down? how the system should behave?</li> <li>It will have message loss?</li> </ul> <p>Exponential backoff and Jitter:\u00a0https://aws.amazon.com/pt/blogs/architecture/exponential-backoff-and-jitter/</p> <p>OTEL -\u00a0https://opentelemetry.io/</p>"},{"location":"microservices/#choreography-vs-orchestration","title":"Choreography vs Orchestration","text":"<p>A choreographed system uses by definition event-driven communication, whereas microservice orchestration uses command-driven communication. An event is something which happened in the past and is a fact. The sender does not know who picks up the event or what happens next. An example can be the event \u201cCredit checked.\u201d</p> <p>Choreography</p> <p>All microservices talks to each other, there isn't a master service to orchestrate the communication, if service A needs something from service C it will talk directly.</p> <ul> <li>Loose coupling: Choreography allows microservices to be loosely coupled, which means they can operate independently and asynchronously without depending on a central coordinator. This can make the system more scalable and resilient, as the failure of one microservice will not necessarily affect the other microservices.</li> <li>Ease of maintenance: Choreography allows microservices to be developed and maintained independently, which can make it easier to update and evolve the system.</li> <li>Decentralized control: Choreography allows control to be decentralized, which can make the system more resilient and less prone to failure.</li> <li>Asynchronous communication: Choreography allows microservices to communicate asynchronously, which can be more efficient and scalable than synchronous communication.</li> </ul> <p>Orchestration</p> <p>Has a \"maestro\" that control the moment where each step/service will execute.</p> <p>When talking about orchestration you can picture a big orchestra which features multiple instruments and a conductor who makes sure that everyone stays in tact. He tells when which instrument needs to play to ensure that the song sounds as it should like.</p> <ul> <li>Simplicity: Orchestration can be simpler to implement and maintain than choreography, as it relies on a central coordinator to manage and coordinate the interactions between the microservices.</li> <li>Centralized control: With a central coordinator, it is easier to monitor and manage the interactions between the microservices in an orchestrated system.</li> <li>Visibility: Orchestration allows for a holistic view of the system, as the central coordinator has visibility into all of the interactions between the microservices.</li> <li>Ease of troubleshooting: With a central coordinator, it is easier to troubleshoot issues in an orchestrated system.</li> </ul>"},{"location":"microservices/#durs-principle","title":"DURS Principle","text":"<p>Each service can be independently DURS (deployed, updated, replaced, and scaled)</p> <ul> <li>Microservice</li> <li>Domain-Driven Design</li> <li>Failure Isolation</li> <li>Continuous Delivery</li> <li>Decentralization</li> <li>DevOps</li> <li>Scalability</li> <li>Resilience</li> </ul>"},{"location":"microservices/#failure-isolation","title":"Failure Isolation","text":"<ul> <li>What happens when that request fails?</li> <li>What is our average response time on that request?</li> <li>What would our support team change about the user experience?</li> </ul> <p>The microservices architecture moves application logic to services and uses a network layer to communicate between them. Communicating over a network instead of in-memory calls brings extra latency and complexity to the system which requires cooperation between multiple physical and logical components. The increased complexity of the distributed system leads to a higher chance of particular\u00a0network failures.</p> <p>One of the biggest advantage of a microservices architecture over a monolithic one is that teams can independently design, develop and deploy their services. They have full ownership over their service\u2019s lifecycle. It also means that teams have no control over their service dependencies as it\u2019s more likely managed by a different team. With a microservices architecture, we need to keep in mind that provider\u00a0services can be temporarily unavailable\u00a0by broken releases, configurations, and other changes as they are controlled by someone else and components move independently from each other.</p> <p>Strategies: </p> <ul> <li>Automatic Rollouts</li> </ul> <p>In a microservices architecture, services depend on each other. This is why you should minimize failures and limit their negative effect. To deal with issues from changes, you can implement change management strategies and\u00a0automatic rollouts.</p> <p>For example, when you deploy new code, or you change some configuration, you should apply these changes to a subset of your instances gradually, monitor them and even automatically revert the deployment if you see that it has a negative effect on your key metrics.</p> <p>Another solution could be that you run two production environments. You always deploy to only one of them, and you only point your load balancer to the new one after you verified that the new version works as it is expected. This is called blue-green, or red-black deployment.</p> <ul> <li>Health-check and Load balancing</li> </ul> <p>Instances continuously start, restart and stop because of failures, deployments or autoscaling. It makes them temporarily or permanently unavailable. To avoid issues, your load balancer should\u00a0skip unhealthy instances\u00a0from the routing as they cannot serve your customers\u2019 or sub-systems\u2019 need.</p> <p>Application instance health can be determined via external observation. You can do it with repeatedly calling a\u00a0<code>GET /health</code>\u00a0endpoint or via self-reporting. Modern\u00a0service discovery\u00a0solutions continuously collect health information from instances and configure the load-balancer to route traffic only to healthy components.</p> <p>more:</p> <ul> <li>designing-microservices-architecture-for-failure</li> </ul>"},{"location":"rabbitmq/","title":"RabbitMQ","text":"<p>RabbitMQ is an open-source message broker software, highly consolidated, used to work with communication between systems. Operating asynchronously, it acts as an intermediary that processes our messages between producers and consumers, in addition to having queues that have different forwarding options. It is widely used in distributed systems to enable communication between applications and services.</p>"},{"location":"rabbitmq/#features","title":"Features","text":"<ul> <li>Message Broker;</li> <li>Implements AMQP, MQTT, STOMP and HTTP;</li> <li>Developed in Erlang;</li> <li>Allow decoupling between services;</li> <li>Fast and Powerful; Stores data in memory;</li> <li>Durability, the messages can be stored in memory or on disk, depending on the configuration.</li> <li>Scalability</li> <li>Priority, which means that messages with higher priority are processed first.</li> <li>Many companies use it, well tested.</li> </ul>"},{"location":"rabbitmq/#under-the-hood","title":"Under the Hood","text":"<p>RabbitMQ is based on the message queue model, where messages are sent from producers (publishers) to consumers through a broker (RabbitMQ server). </p> <p>It only opens a single persistent connection (which speeds up the process as it doesn't need to create TCP connections all the time) and then configures the channels (sub-connection) for consumers to retrieve the data they need;</p> <p>1 Thread per channel; New thread for new channel;</p> <p>Basic Functioning:</p> <p>Publisher \u21d2 Publishes the message for whoever wants to consume it. It sends messages to an exchange, which routes the messages to the appropriate queue.</p> <p>Consumer \u21d2 Retrieves messages from a queue.</p> <p>Queue \u21d2 The Publisher does not send a message directly to the consumer. Instead, the message falls into a queue, and the consumer reads that queue of messages, and each message they read is deleted. Multiple consumers can be connected to the same queue, and messages are distributed to consumers in a round-robin fashion by default. Basically, is a routing agent that receives messages from producers and routes them to one or more queues.</p> <p>Exchange \u21d2 Retrieves the message the publisher sent and discovers which queue the message will be sent to, because in some cases, the message may go to more than one queue.</p> <p>Flow:</p> <p>[Publisher] \u21d2 [Exchange] \u21d2 [Queue] \u21d2 [Consumer]</p> <p>The publisher publishes a message, the exchange redirects it to the queue(s), and the consumer listens to the queue to process the message and delete it afterwards.</p>"},{"location":"rabbitmq/#types-of-exchange","title":"Types of Exchange","text":"<ul> <li>Direct: The Exchange sends the message specifically to a certain queue</li> <li>Fanout: The exchange sends the message to all the queues that are binded/related to this exchange, if there are 10 related queues the message is sent to the 10 queues</li> <li>Topic: It has rules, e.g.: depending on the 'route key' it will forward to the queue we want.</li> <li>Headers: In the message header, we determine which queue we want the exchange to deliver to. It's not very common to use.</li> </ul> <p>The system can have several exchanges with several queues and each queue can be related to one or more exchanges.</p> <p>Playground: http://tryrabbitmq.com/</p>"},{"location":"rabbitmq/#direct-exchange","title":"Direct Exchange","text":"<p>Uses a message routing key to transport messages to queues. The routing key is a message attribute that the producer adds to the message header. You can consider the routing key to be an \u201caddress\u201d that the exchange uses to determine how the message should be routed. A message is delivered to the queue with the binding key that exactly matches the message\u2019s routing key</p> <p>The direct exchange\u2019s default exchange is \u201camq. direct\u201c, which AMQP brokers must offer for communication</p> <p>As is shown in the figure, queue A (create_pdf_queue) is tied to a direct exchange (pdf_events) with the binding key \u201cpdf_create\u201d. When a new message arrives at the direct exchange with the routing key \u201cpdf_create\u201d, the exchange sends it to the queue where the binding key = routing key; which is queue A in this example (create_pdf_queue).</p> <p></p> <p>The exchange binds the related queue with an exchange so it can forward the messages The message passes the Routing Key and the Exchange redirects to the respective queue.</p> <p></p>"},{"location":"rabbitmq/#fanout-exchange","title":"Fanout Exchange","text":"<p>A fanout exchange, like direct and topic exchange, duplicates and routes a received message to any associated queues, regardless of routing keys or pattern matching. Here, your provided keys will be entirely ignored. </p> <p>Fanout exchanges are useful when the same message needs to be passed to one or perhaps more queues with consumers who may process the message differently</p> <ul> <li>A single message goes to all queues;</li> <li>Does not have routing key;</li> <li>If we have several consumers consuming the same queue, RabbitMQ will create a kind of loadbalancer and send a distributed message to all</li> </ul> <p>Ex: An e-commerce system has several sectors (purchase, marketing, invoice, log, ...)</p> <p>If we want that when a user makes a purchase the message is sent to all queues we can use this model, the queues are connected in the exchange and we send the messages.</p> <p></p>"},{"location":"rabbitmq/#topic-exchange","title":"Topic Exchange","text":"<p>Sends messages to queues depending on wildcard matches between the routing key and the queue binding\u2019s routing pattern. Messages are routed to one or more queues based on a pattern that matches a message routing key.</p> <ul> <li>Routing keys have rules, similar to Regex; Ex: X.LOG, T.Y, ...</li> </ul> <p></p>"},{"location":"rabbitmq/#queues","title":"Queues","text":"<ul> <li> <p>FIFO \u21d2 First In, First Out     The message that enters first is the first to leave.</p> </li> <li> <p>Properties:</p> <ul> <li>Durable: If it should be saved even after the broker restarts, persist on disk or keep it only in memory. If we restart the broker and the queue is not durable, it will be removed.</li> <li>Auto-delete: Automatically removed when the consumer disconnects. If the consumer disconnects, the queue will be deleted.</li> <li>Expiry: Defines the time that no messages or clients are consuming. For example, if no client is consuming this queue for 3 hours, it will be deleted.</li> <li>Message TTL: Message's lifetime, if not consumed during this time, the message will be automatically removed.</li> <li>Overflow:<ul> <li>Drop head (remove the oldest one), if it reaches the limit of messages for this queue and a new one arrives, remove the oldest one.</li> <li>Reject publish, does not allow more inputs; will receive an error.</li> </ul> </li> <li>Exclusive: Only the channel that created it can access it.</li> <li>Max Length or bytes: Maximum number of messages or maximum size in bytes allowed. For example, the queue can have a maximum of 10 messages. After that, it will fall under the overflow rule, or the queue can only have 2MB of messages.</li> </ul> </li> </ul>"},{"location":"rabbitmq/#dead-letter-queues","title":"Dead Letter Queues","text":"<p>Some messages can't be delivered for any reason, no one read or processed them. After that, we can configure them to fall into a specific exchange that routes messages to a dead letter queue.</p> <p>Such messages can be consumed and examined later.</p>"},{"location":"rabbitmq/#lazy-queues","title":"Lazy Queues","text":"<p>When the message flow is too large for consumers to process everything, and the RabbitMQ memory limit is reached, these messages are stored on disk to ensure that more messages can arrive and not be lost.</p> <ul> <li>It requires high I/O, so it is more costly.</li> <li>When there are millions of messages in a queue, for any reason, there is the possibility of freeing up memory by specifically moving the messages from the relevant queue to disk.</li> </ul>"},{"location":"rabbitmq/#reliability","title":"Reliability","text":"<ul> <li>How to ensure that messages will not be lost along the way?</li> <li>How to ensure that messages could be correctly processed by consumers?</li> <li>What if each message was worth 1 million dollars?</li> </ul> <p>RabbitMQ features to solve such situations:</p> <ul> <li>Consumer acknowledgement \u21d2 consumer confirms receiving the message;</li> <li>Publisher confirms \u21d2 ensure that the message has reached the exchange;</li> <li>Durable/persisted queues and messages \u21d2 should not be used always because it costs memory and will slow down the system</li> </ul> <p>Types of Consumer Acknowledgement</p> <ul> <li> <p>Basic.Ack     Every time we send a message to the consumer and it responds with \"I received and processed the message\"</p> </li> <li> <p>Basic.Reject     If the consumer receives the message and can't resolve it (throw an exception), the message goes back to the queue because the consumer couldn't resolve it.</p> </li> <li> <p>Basic.Nack     Same as Reject, but can reject more than one message at the same time.</p> </li> </ul> <p>Publisher Confirms</p> <p>When we want to make sure that the message is going to the exchange,</p> <p>In this case, the message has an ID (we pass this integer ID in the message, ex 1), when the message receives this message, it returns to the publisher saying it received the message with ID 1. Ack: ID = 1</p> <p>If for some reason, the exchange has some internal problem, it will tell the publisher that it could not process the message, Nack: ID = 1</p> <p>For important messages, we should use Publisher confirms.</p>"},{"location":"rabbitmq/#use-cases","title":"Use Cases","text":"<ul> <li>Microservices: Used to enable communication between services. Each service can send and receive messages through RabbitMQ, which provides a scalable and reliable messaging solution.</li> <li>Real-time data processing: Can be used for real-time data processing, such as stream processing, event sourcing, and complex event processing (CEP). RabbitMQ can handle large volumes of messages and provide low-latency processing.</li> <li>Message-driven architecture: Messages can trigger actions in the system. For example, a message could trigger a workflow, a notification, or an update to a database.</li> <li>IoT: It can be used to enable communication between devices and services.</li> </ul>"},{"location":"rabbitmq/#refs","title":"Refs","text":"<ul> <li>rabbitmq-exchange-type</li> </ul>"},{"location":"Architecture/architectural-features/","title":"Architectural Features","text":""},{"location":"Architecture/architectural-features/#operational","title":"Operational","text":"<ul> <li>Availability;<ul> <li>Will it be online 24/7? What level of availability do I want in my system? Should it be available in other countries?</li> </ul> </li> <li>Disaster recovery;<ul> <li>How will I recover when my system is down? What if an AWS region goes down? Do I need multi-cloud?</li> <li>Processes must be in place for these cases;</li> </ul> </li> <li>Performance;<ul> <li>How much performance do I want in this system? 5000 requests per second? It will affect the choice of the database, replicas, ...</li> </ul> </li> <li>Recovery (backup);<ul> <li>Test the backup periodically to make sure it's working.</li> </ul> </li> <li>Reliability and security;<ul> <li>Prevent brute force attacks, password policies, captcha, ...</li> </ul> </li> <li>Robustness;<ul> <li>Can it scale? Can it change region if one goes down? Do you have the necessary amount of IP, CIDR?</li> </ul> </li> <li>Scalability;</li> </ul>"},{"location":"Architecture/architectural-features/#structural","title":"Structural","text":"<p>Characteristics of my software for it to function in a flexible way;</p> <ul> <li>Configurability<ul> <li>Use environment variables, abstractions, there should be no need to change the source code to deploy in different environments (dev, stage, prod).</li> </ul> </li> <li>Extensibility<ul> <li>It should be able to grow in a way that things can be plugged into it, for example: Payment Gateway, changing from gateway x to y should be simple, work with interfaces, abstractions. The application should not depend on vendors (databases, gateways, modules, etc.).</li> </ul> </li> <li>Easy installation<ul> <li>Standardization of the environment, Infrastructure as a Code, docker compose for development.</li> </ul> </li> <li>Component reuse<ul> <li>Attach libraries so that all services can use them, for example: npm packages.</li> </ul> </li> <li>Internationalization<ul> <li>For example, if the payment gateway changes, how will the conversion be handled? How about installment payments, etc.?</li> </ul> </li> <li>Easy maintenance<ul> <li>Make the software simple, SOLID, understand the layers of the system. Fixing bugs and adding new features should become easy.</li> </ul> </li> <li>Easy support (logs, debugging)<ul> <li>Standardization in log generation, observability, etc.</li> </ul> </li> </ul>"},{"location":"Architecture/architectural-features/#cross-cutting","title":"Cross-Cutting","text":"<ul> <li>Accessibility<ul> <li>Is it easy for others to access the site? What about people with disabilities? Use appropriate labels, lightweight images, etc.</li> </ul> </li> <li>Data retention and recovery process (how long will the data be kept)<ul> <li>Storage is expensive. Do the data you have today really need to exist for 7, 30, or 60 days? Old data can be kept in different locations after a certain time to make the database cheaper.</li> </ul> </li> <li>Authentication and Authorization<ul> <li>In a distributed architecture, how will it work? Use Keycloak? API Gateway?</li> </ul> </li> <li>Legal<ul> <li>How long will the data be kept? Will it be encrypted?</li> </ul> </li> <li>Privacy<ul> <li>How can you minimize problems related to user data leakage under LGPD? Avoid developers having access to the production database. Separate sensitive information into separate databases. Have a database with fake data for development?</li> </ul> </li> <li>Security<ul> <li>Use web firewall and work with mechanisms that can identify robots, etc.</li> </ul> </li> <li>Usability<ul> <li>How is the API organized? Is there documentation? Is there a readme? Who is my client?</li> </ul> </li> </ul>"},{"location":"Architecture/design-quality-software/","title":"Designing Quality Software","text":""},{"location":"Architecture/design-quality-software/#performance","title":"Performance","text":"<p>When it comes to developing software, performance is an essential factor. It's the software's ability to complete a specific workload. But how can we determine how well a software performs a particular action? To measure performance, we need data. The primary units of measurement for evaluating software performance are latency and throughput.</p> <p>The units of measurement for evaluating software performance are:</p> <ul> <li>Latency or \"response time\" \u21d2 Time it takes for a software application to process a request and complete a workload.</li> <li>Throughput \u21d2 Number of requests a software application can handle.</li> </ul> <p>Having a performative software is different from having a scalable software</p> <p>Objective:</p> <ul> <li>Reduce latency, usually measured in milliseconds; It is affected by the application's processing time, network, and external calls;</li> <li>Increase throughput, allow the software to handle more requests; It is directly linked to latency;</li> </ul> <p>Main reasons for low performance:</p> <ul> <li>Inefficient processing;<ul> <li>Ex: Algorithms, the way the application is handling data, poorly made queries, etc.</li> </ul> </li> <li>Limited computational resources;<ul> <li>CPU, RAM, Memory,...</li> </ul> </li> <li>Working in a blocking way;<ul> <li>Every time you make a request and if that request blocks the application to deal specifically with that request, the application will decrease throughput because it will not be able to handle thousands of requests in parallel.</li> <li>Separate each request into a thread.</li> </ul> </li> <li>Serial access to resources;<ul> <li>Every time you access an API you wait for it to finish and call the next one, one after the other, this decreases throughput.</li> </ul> </li> </ul> <p>Main ways to increase efficiency:</p> <ul> <li>Scale of computational capacity (CPU, Disk, Memory, Network);</li> <li>Logic behind the software (Algorithms, queries, framework overhead);</li> <li>Concurrency and parallelism; Dealing with multiple processes at the same time;</li> <li>Databases (types of databases, schema);</li> <li>Caching;</li> </ul> <p>Computational Capacity: Vertical Scale vs Horizontal Scale</p> <ul> <li> <p>Vertical Scale \u21d2 Increase the computational capacity of the machine, so that the application can receive more requests;</p> </li> <li> <p>Horizontal Scale \u21d2 Increase the number of machines by placing a load balancer in front to balance the loads;</p> </li> </ul> <p>Concurrency and Parallelism</p> <p>\"Concurrency is about dealing with many things at once. Parallelism is about doing many things at once.\"</p> <ul> <li> <p>Concurrency \u21d2 Deals with several things but they are not necessarily executed at the same time; When two or more tasks can start to be executed and finish in overlapping time periods, not meaning that they need to be in execution necessarily at the same time.</p> <p>That is, you have concurrency when:</p> <ul> <li> <p>More than one task progresses at the same time in an environment with multiple CPUs/cores;</p> </li> <li> <p>Or in the case of a single-core environment, two or more tasks may not progress at the exact same moment, but more than one task is processed in the same time interval, not waiting for one task to complete before starting another.</p> </li> </ul> </li> <li> <p>Parallelism \u21d2 Perform actions simultaneously. It happens when two or more tasks are executed, literally, at the same time. Obviously requires a processor with multiple cores, or multiple processors so that more than one process or thread can be executed simultaneously.</p> </li> </ul> <p>Imagining a web server with a worker:</p> <p>Serial way - Working in a serial way - single process - Serving 5 requests</p> <p>If the worker operates in serial mode, it will attend to one request at a time.</p> <pre><code>10ms -&gt; 10ms -&gt; 10ms -&gt; 10ms -&gt; 10ms \n------------------------------------\n              50ms\n</code></pre> <p>Concurrent/parallel way - 5 threads serving 1 request each</p> <p>If it operates in parallel mode, it can handle multiple requests simultaneously, which improves its performance.</p> <pre><code>10 ms -&gt;\n10 ms -&gt;\n10 ms -&gt;\n10 ms -&gt;\n10 ms -&gt;\n--------\n  10ms\n</code></pre>"},{"location":"Architecture/design-quality-software/#caching","title":"Caching","text":"<p>Fetching things that have been processed before and returning them more quickly to the end user.</p> <ul> <li> <p>Edge caching / Edge computing</p> <p>The user does not hit the machine (ec2, ecs, lambda,...) because this type of cache will cache the data on an edge that is before the main server. Examples of services: Cloudflare, Cloudfront.</p> <p>Cloudfront caches static files (images, css, js, ...) in an availability zone and users who are close to them will receive the data almost instantly.</p> <p>Types of edge cache:</p> <ul> <li>Static data;</li> <li>Web pages;</li> <li>Internal functions;<ul> <li>Avoid reprocessing heavy algorithms by setting a TTL of 30min, for example;</li> <li>Access to the database (avoid unnecessary I/O).</li> </ul> </li> <li>Objects;</li> </ul> <p>Problem: Netflix has millions of accesses to its web application, if the data center is in the United States, users from all over the world will have to travel from their location to the USA to get these terabytes of data in order to access the site, congesting Netflix's network and the internet in general and generating a lot of latency to access content.</p> <p>Solution: Edge computing:  caching this data in the regions where your users are located, avoiding users having to travel across the entire internet to hit the data center. It will give a better experience to the user (low latency) and we will pay less because we will have fewer requests on our servers.</p> <ul> <li>Cache performed closer to the user. The user does not need to hit the USA to get an image.</li> <li>Avoids the request reaching the Cloud Provider / Infra.</li> <li>Usually static files. (Cheaper, faster)</li> <li>CDN - Content Delivery Network; (Network of servers, spreads your content to other data centers), e.g: Akamai's CDN \u2192 More than 500 points in Brazil, spreads your content (videos) to the regions of Brazil.     If you're in Portugal, your video should be loaded from a CDN in Portugal.     Cost:<ul> <li>CDN cost</li> <li>Distribution cost: If Akamai does not have the video/image on its CDN, it will fetch the content from the server (e.g: us-east-1), download it and make it available to you. Once it has been downloaded, it will perform something called Midgress, which means that it will now take that content and distribute it among strategic points, and this content spreading consumes bandwidth, which you pay for to spread across the edges. Origin \u2192 CDN \u2192 midgress \u2192...</li> </ul> </li> <li>Cloudflare workers \u21d2 Edge computing platform</li> <li>Vercel</li> <li>Akamai</li> </ul> </li> </ul>"},{"location":"Architecture/design-quality-software/#scalability","title":"Scalability","text":"<p>Scalability is the ability of systems to support an increase (or decrease) in workloads by incrementing (or reducing) the cost in equal or lesser proportion.</p> <p>While performance focuses on reducing latency and increasing throughput, scalability aims to have the possibility of increasing or decreasing throughput by adding or removing computational capacity.</p> <p>Vertical Scaling - Horizontal Scaling</p> <p>Scaling Software - Decentralization</p> <p>Machines are disposable, so your system must be independent of the machine being used.</p> <ul> <li>Ephemeral Disk \u2192 Everything you save on disk can be deleted when needed. We need the power to kill the machine when we want to.</li> <li>Application Server vs Assets Server \u2192 Server has the application code, assets are on an assets server (s3 bucket).</li> <li>Centralized Cache \u2192 The cache is not on the machine itself, it is on a server specific for caching. ex: DynamoDB.</li> <li>Centralized Sessions \u2192 The software cannot store state.</li> <li>Upload / File Writing \u2192 Bucket / file server</li> </ul> <p>Everything can be easily destroyed and created.</p> <p>Scaling Databases</p> <ul> <li>Increasing computational resources; More disk, memory, cpu,...;</li> <li>Distributing responsibilities (write vs read); Create replicas;</li> <li>Sharding horizontally; Create multiple read machines;</li> <li>Query and index optimization;</li> <li>Serverless / Cloud databases; (Dynamo, Aurora, Fauna,...);</li> </ul> <p>Use an APM (Application performance monitoring) system to identify bottlenecks in queries and problems that are occurring.</p> <ul> <li>Explain in queries \u2192 identify what is happening in queries</li> <li>CQRS (Command Query Responsibility Segregation)</li> </ul> <p>Reverse Proxy</p> <p>Proxy = Proxy, a person who speaks on your behalf</p> <p>Redirects users;</p> <p>A proxy is a service that routes traffic between the client and another system. It can regulate traffic according to present policies, enforce security, and block unknown IPs.</p> <p>A reverse proxy is a server that sits in front of all servers, it has rules, and these rules cause it to forward your request to certain servers that can resolve it. Unlike a normal proxy, which sits on the client side, the reverse proxy is designed to protect the servers. It accepts the client's request and forwards the request to one or more servers and returns the processed result. The client communicates directly with the reverse proxy and does not know about the server that processed it.</p> <p>Reverse Proxy Solution:</p> <ul> <li>Nginx</li> <li>HAProxy (HA = High Availability)</li> <li>Traefik</li> </ul>"},{"location":"Architecture/design-quality-software/#resilience","title":"Resilience","text":"<p>It is a set of intentionally adopted strategies for adapting a system when a failure occurs.</p> <p>In software, if an error occurs, it either throws an exception or has a strategy to serve the client request, even if partially, despite the error.</p> <p>Resilience is the power to adapt if something happens.</p> <p>How can I register a user even if the Correios API is not working to get the ZIP code? How can I get that sale if my payment service is not working?</p> <p>The only thing we are certain of is that our software will fail at some point, but we must have resilience to deal with these failures.</p> <p>Having resilience strategies allows us to minimize the risks of data loss and important business transactions.</p>"},{"location":"Architecture/design-quality-software/#resilience-strategies","title":"Resilience Strategies","text":"<ul> <li> <p>Protect and be Protected</p> <p>It is very common today for an application to be based on an ecosystem with several other services. A system in a distributed architecture needs to adopt self-preservation mechanisms to ensure its operation with the highest possible quality.</p> <p>A system cannot be \"selfish\" to the point of making more requests on a system that is failing.</p> <p>Example:</p> <p>System A sending a request to know the price table for System B, but System B does not respond, and System A keeps sending several requests in succession, eventually System B will go offline.</p> <p>So, a system cannot keep sending multiple requests to another until it crashes, it is no use kicking a dead dog.</p> <p>A slow system online is often worse than a system offline. (Domino effect).</p> <p>Example:</p> <p>System A calls System B that calls System C, if System C is slow, it will delay B's response, which will consequently delay A's response, and if more requests are coming, it will end up crashing some service and eventually maybe all.</p> <p>So, sometimes if a system is not handling requests, it is better to return 500 for everyone until it stabilizes again.</p> <p>One form of protection is to identify that you cannot handle requests or that the other service is no longer able to handle them or taking too long to respond.</p> </li> <li> <p>Health Check</p> <p>Check the application's health;</p> <p>\"Without vital signs, it is not possible to know the health of a system\".</p> <p>Important to know if it is able to receive requests or not, ..., if it is bad, it already returns a 500 error for the other applications to know if it is unavailable.</p> <p>An unhealthy system has a chance to recover if traffic stops being directed to it temporarily.</p> <p>Self-healing, give the server time to self-recover, stop sending requests to it.</p> <p>Quality Health Check, usually people only put a /health route to check if the application is healthy, but this cannot measure if the application is really healthy because in the other routes, there is usually processing, database query, data formatting, etc.</p> </li> <li> <p>Rate Limiting</p> <p>Protects the system based on what it was designed to support.</p> <p>To know this limit, we can do stress testing and/or the company's budget to know how much traffic it can handle.</p> <p>Example: 100 requests per second, \"it's as much as I can handle\", returns a 500 error if exceeded.</p> <p>We can also work with priority, reserve 60 requests for an important client and 40 for the rest.</p> </li> <li> <p>Circuit breaker</p> <p>It protects the system by denying requests made to it. Example: 500</p> <p>If the API is having problems, the circuit opens the \"switch\" and no longer allows requests to pass through.</p> <p>Closed Circuit = Requests arrive normally;</p> <p>Open Circuit = Requests do not reach the system. Instant error to the client;</p> <p>Half-open = Allows a limited number of requests to verify if the system can fully come back online.</p> </li> <li> <p>API Gateway</p> <p>Centralizes the receipt of all application requests, all requests pass through it and it applies the policy/validation rules before forwarding the request to the resolver.</p> <p>It understands the individual needs of each application.</p> <p>Ensures that \"inappropriate\" requests reach the system:</p> <p>Example: Unauthenticated user.</p> <p>We take the authentication responsibility away from the application and pass it to the API Gateway, just by doing that we already reduce the resource expense to check whether the user is authenticated or not.</p> <p>It's like living in a condominium and the person can only enter the apartment if they go through the gatehouse (API Gateway) first.</p> <p>Implements policies such as Rate Limiting, Health check, etc...</p> <p>Example of services: Kong</p> </li> <li> <p>Service mesh</p> <p>Controls network traffic.</p> <p>Instead of services communicating directly with each other, they hit a proxy called a sidecar and that sidecar sends the request to the sidecar of another system.</p> <p><code>[ System A ] (Sidecar) ----&gt; (Sidecar) [System B]</code></p> <p>All network communication is performed via proxy, so everything that is passing on the network can be controlled.</p> <p>Avoids protection implementations by the system itself.</p> <p>You can analyze everything that is happening on the network, all traffic, and with that you can define the rules for Rate Limiter, Circuit Breaker, etc...</p> <p>mTLS \u2192 encrypt the network to ensure that Service A is itself and B is itself.</p> </li> <li> <p>Asynchronous communication</p> <p>With less computational resources, it can handle more computational resources than it could otherwise.</p> <p>It does not need to deliver the response to the requests immediately.</p> <p>Avoids data loss;</p> <p>There is no data loss in sending a transaction if the server is down;</p> <p>Example: synchronously, if we make a payment and the server is not available, it will tell us to try again later, however, asynchronously allows us to send the message/information to an intermediary (Redis, RabbitMQ, AWS SNS, Kafka, ...) to handle the action because the other end does not need the response at that moment.</p> <p>The server can process the transaction in its own time when it is online;</p> </li> <li> <p>Delivery guarantees with Retry</p> <p>When we want resilience in the request, we need to make sure that the message we are sending is reaching its destination, but not always the message can reach its destination because the other system may be slow, offline, ... and because of that one of the ways to have resilience and minimize this problem is to have Retry policies, send a message, if the system does not respond, send another, and another, ...</p> <p>But we have a problem, if 10 services send a request and the server cannot handle 10 simultaneous requests, even if the other servers have retry policies and keep sending messages every 2 seconds, it will not help because they are all trying to access the server at the same time and will always return an error.</p> <p>To do this, we have the Exponential backoff - Jitter, where we wait exponentially 2s, 4s, 8s, 16s, etc., to give the server more time to recover, and with Jitter, it has an algorithm that adds random noise to requests so that they do not happen simultaneously. For example, (2.1s, 2.5s, 2.0s), (4.7s, 4.01s, 4.12s), etc., so that even if services are sending requests simultaneously, the noise will make them arrive differently.</p> </li> <li> <p>Delivery guarantees with Kafka</p> </li> <li> <p>Complex situations</p> <ul> <li>What happens if the message broker goes down? (Kafka, RabbitMQ, SNS, etc.)</li> <li>Will messages be lost?</li> <li>Will your system be down?</li> <li>How to ensure resilience in unusual situations?</li> </ul> <p>There are always some Single Points of Failure (SPF).</p> <p>For example, if all the resilience you rely on is in Apache Kafka, it means that your SPF is in Apache Kafka.</p> <p>How can we avoid this, so that if Kafka goes down, the system does not lose information?</p> <p>All of this involves risk management, and the more resilience we guarantee, the more expensive it will be for the company.</p> <p>If AWS goes down? Is it worth working with multi-cloud?</p> <p>There will always be a limit to your resilience, and the more resilience, the more effort and cost.</p> <p>The responsibility for defining the level of resilience of the system lies with the CTO / CEO, who defines the risk that it will generate for the business.</p> </li> </ul>"},{"location":"Architecture/sustainability/","title":"Sustainability","text":"<ul> <li>Developing software is expensive;</li> <li>Software solves a \"pain\";</li> <li>Software needs to pay for itself over time;<ul> <li>The company should profit more because of that software;</li> <li>The software, along with the company, is an organism, as this evolution occurs, the software must be able to sustain itself in a way that the cost is lower than the result it is bringing to the company;</li> </ul> </li> <li>Long-term thinking;<ul> <li>It is very common for a software to reach a point where the developers are demotivated to work and want to start everything from scratch, but often the software hasn't even paid for itself and has to start over, in this way the software will never be able to reach the breakeven point, balance between cost vs what it generates.</li> </ul> </li> <li>Follow the evolution of the business;<ul> <li>For example: imagine if Nubank goes through a refactoring process and customers have to wait for the bugs to be fixed before they can use the product again, this is unfeasible for the business, the software must be built with a long-term perspective and always following the evolution of the business.</li> </ul> </li> <li>The longer the software stays up, the more return it generates;</li> <li>The solution needs to be architected;</li> </ul>"},{"location":"Architecture/sustainability/#software-architecture-pillars","title":"Software Architecture Pillars","text":"<ul> <li>Structuring     -Easy evolution and componentization to meet business objectives;</li> <li>Componentization;</li> <li>Relationship between systems; (External communication, third-party services, networks)</li> <li>Governance; (Standardization, rules, documentation, protocols, ...)<ul> <li>A software must be independent of the developer. If a person leaves, it should be easy for someone else to take on that role and continue the project in the same way it was.</li> </ul> </li> </ul>"},{"location":"Architecture/sustainability/#architectural-requirements","title":"Architectural Requirements","text":"<ul> <li>Performance<ul> <li>Example: Handling 500 requests per second, resilience, etc.</li> </ul> </li> <li>Data storage<ul> <li>Example: Data centers in Brazil, Europe, etc.?</li> </ul> </li> <li>Scalability<ul> <li>Horizontally, vertically, load balancer, etc.</li> </ul> </li> <li>Security<ul> <li>SSL, Rate Limiter, Mutual TLS, etc.</li> </ul> </li> <li>Legal<ul> <li>Comply with the legislation of each country (LGPD, Terms of Use, Retention Period for Data, etc.);</li> </ul> </li> <li>Marketing<ul> <li>Track where each request comes from, etc.</li> </ul> </li> </ul>"},{"location":"Architecture/types-of-architecture/","title":"Types of Architecture","text":"<ul> <li>Technological Architecture</li> <li>Enterprise/Corporate Architecture</li> <li>Solution Architecture</li> <li>Software Architecture</li> </ul>"},{"location":"Architecture/types-of-architecture/#technological-architecture","title":"Technological Architecture","text":"<p>This type of architecture focuses on specific technologies used in the market. A technology architect has in-depth knowledge of a particular tool or technology and can generate value based on their expertise. They are well-versed in the workings of a particular technology, and are able to recommend and implement solutions that leverage that technology. Examples of technology architects include Elastic architects (for Kibana and Elastic Search), Java architects, SQL Server architects, SAP architects...</p> <ul> <li>Specialization in specific market technologies, has a great knowledge of a technology/tool.</li> <li>Generate value based on specialties;</li> <li>In-depth knowledge of the technology;</li> </ul>"},{"location":"Architecture/types-of-architecture/#enterprise-architecture","title":"Enterprise Architecture","text":"<p>Enterprise architecture involves creating policies and rules that strategically impact the organization as a whole. In large companies with thousands of employees, the number of technologies that can be utilized may be endless. An enterprise architect brings solid governance to the infrastructure and technology that will be used, with a focus on evaluating costs that make sense for the company's growth. They evaluate licenses, assess new technologies, standardize technologies, plan large-scale implementations, and oversee core systems and migrations (such as from monolithic to microservices).</p> <ul> <li>Policies and rules that strategically impact the organization as a whole,</li> <li>The number of technologies that can be used can reach N, this architect brings solid governance regarding the infrastructure/technology that will be used with an evaluation of costs that make sense for the company to grow.</li> <li>Evaluation of licenses;</li> <li>Evaluation of possible new technologies;</li> <li>Standardization of technologies;</li> <li>Planning for large deployments (SAP, Salesforce,...)</li> <li>Core systems, migrations (Monolithic, Microservices, ...)</li> </ul>"},{"location":"Architecture/types-of-architecture/#solutions-architecture","title":"Solutions Architecture","text":"<p>Solution architecture sits at the intersection of business and software, translating business requirements into software solutions. Solution architects create architectural designs that map out how the software will function. They use tools like the C4 diagram, UML, and BPMN to analyze the commercial impact of technology choices, assess the company's context and team, and make informed decisions. For example, they may assess whether it makes sense for a company to switch from AWS to Google Cloud Platform, or whether migrating from Oracle to SQL or Postgres would be cost-effective. Solution architects take a short, medium, and long-term view of the software, and may be involved in pre-sales and sales processes to better understand the business and its needs. They also analyze the cost impact of implementing solutions, bringing predictability to the project.</p> <ul> <li>Between the business and software area, transforms business requirements into software solutions;</li> <li>Architectural designs of the software solution to reproduce how it will work;</li> <li>C4 diagram, UML, BPMN;</li> <li>Analyzes commercial impacts regarding a technology choice, analyzes the company's context and team to be able to make that decision (ex: if the company is using AWS, does it make sense to switch to Google just because it's the technology your team knows? Is there a financial advantage in this?, Oracle database, does it make sense to switch to SQL, Postgres,...?);</li> <li>Short, medium, long-term thinking;</li> <li>May participate in the pre-sales and sales process, usually accompanies the client to better understand the business;</li> <li>Analyzes the cost impacts for the business, how much does it cost to implement this CRM? brings predictability</li> <li>High-level vision (macro);</li> </ul>"},{"location":"Architecture/types-of-architecture/#software-architecture","title":"Software Architecture","text":"<p>Software architecture is a discipline of software engineering that is directly tied to the software development process. It has a significant impact on the organizational structure of a company, and involves forming teams and structuring software components. Software architects must consider how to develop the software in components that best meet the business objectives. They must also take into account any restrictions or limitations, such as budget constraints, team capabilities, or technological requirements. Software architecture is the fundamental organization of a system and its components, as well as the principles that guide its design and evolution. A software architect must build a software system with a long-term perspective, and must ensure that the software delivers value to the company. They focus on the micro-level details of design, patterns, tests, implementation, clean architecture, code reviews, and requirements.</p> <ul> <li>Discipline of software engineering, directly linked to the software development process;</li> <li>Directly affects the organizational structure of the company;</li> <li>Formation of teams, structure of software components;</li> <li>How can I develop this software in components? How can these components best meet business objectives? every business has constraints, so I have to adapt my components to these constraints (financial, team, technological, etc.).</li> <li>It is the fundamental organization of a system and its components, their relationships, their environment, as well as the principles that guide its design and evolution.</li> <li>Software must be built with long-term thinking.</li> <li>The role of the software architect is to make the software return value to the company.</li> <li>Micro vision (design, patterns, tests, implementations, clean architecture, code reviews, requirements,....)</li> </ul>"},{"location":"Architecture/types-of-architecture/#role-of-the-software-architect","title":"Role of the Software Architect","text":"<ul> <li>Translate business requirements into architectural patterns;</li> <li>Orchestrate developers and domain experts;</li> <li>Assist in decision-making in times of crisis;</li> <li>Reinforce good development practices.</li> <li>Have a deep understanding of architectural concepts and models;</li> </ul> <p>It is common for us to want to solve a new problem in the same way we solved past ones. For example, if we always used a monolithic pattern, it is very likely that the next challenge will also be a monolithic application. The more the software architect understands architectural models, the greater their range of possibilities to understand and solve the challenge in the best possible way.</p>"},{"location":"Architecture/types-of-architecture/#why-learn-software-architecture","title":"Why learn Software Architecture?","text":"<ul> <li>Ability to navigate from the macro to the micro view of one or more software;</li> <li>Understand the various options we have to develop the same thing and choose the best solution for a given context;</li> <li>Think long-term about the project and its sustainability (Deadlines, People);</li> <li>Make decisions in a more cold and calculated way, thus avoiding being influenced by market \"hypes\";</li> <li>Dive into design patterns and development and their best practices;</li> <li>Have a clearer understanding of the impact that software has on the organization as a whole;</li> <li>Make decisions with more confidence.</li> </ul>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/","title":"Object-Oriented Programming vs Functional Programming","text":"<p>Functional Programming or Object Oriented Programming its more of a personal preference than being a question of which is better than the other.</p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#functional-programming","title":"Functional Programming","text":"<p>Functional programming is the form of programming that attempts to avoid changing state and mutable data. In a functional program, the output of a function should always be the same, given the same exact inputs to the function.</p> <p>ex:</p> <p>It will always be the same result, and has no side effects </p> <pre><code>function sum(a, b) {\n    return a + b\n}\n</code></pre> <p>This is because the outputs of a function in functional programming purely rely on the function's arguments, and there is no magic behind the scenes. This is called eliminating side effects in your code.</p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#oop-object-oriented-programming","title":"OOP (Object Oriented Programming)","text":"<p>Object-oriented programming is a programming paradigm in which you program\u00a0uses objects to represent things\u00a0you are programming about (sometimes real-world things). These objects could be data structures. The\u00a0objects hold data about them in attributes. The attributes in the objects are manipulated through methods or functions that are given to the object.</p> <p>For instance, we might have a\u00a0Person object\u00a0that represents all of the data a person would have: weight, height, skin color, hair color, hair length, and so on. Those would be the attributes. Then the person object would also have\u00a0things that it can do\u00a0such as: pick a box up, put box down, eat, sleep, etc. These would be the functions that play with the data the object stores.</p> <p>The main deal with OOP is the ability to encapsulate data from outsiders. Encapsulation is the ability to hide variables within the class from outside access \u2014 which makes it great for security reasons, along with leaky, unwanted, or accidental usage.</p> <p>Object-Oriented Programming is a diverse programming language that provides a clear structure for a program, making it easier to maintain, manage, debug, and reuse the code. This is perhaps the most significant reason why this programming method is so popular. </p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#major-concepts-of-object-oriented-programming","title":"Major Concepts of Object-Oriented Programming","text":""},{"location":"Computer-Science/OOP-vs-Functional-Programming/#encapsulation","title":"Encapsulation","text":"<p>Encapsulation, in general, is nothing but a fancy word for packaging or enclosing things of interest into one entity. By definition, encapsulation describes the idea of bundling data and methods that work on that data within one unit.</p> <p>Many programming languages use\u00a0encapsulation\u00a0frequently in the form of\u00a0classes. A\u00a0class\u00a0is a program-code-template that allows developers to create an object that has both variables (data) and behaviors (functions or methods). A class is an example of encapsulation in computer science in that it consists of data and methods that have been bundled into a single unit.</p> <p>Encapsulation may also refer to a mechanism of restricting the direct access to some components of an object, such that users cannot access state values for all of the variables of a particular object. Encapsulation can be used to hide both data members and data functions or methods associated with an instantiated class or object.</p> <p>Encapsulation in programming has a few key benefits. These include:</p> <ul> <li>Hiding Data:\u00a0Users will have no idea how classes are being implemented or stored. All that users will know is that values are being passed and initialized.</li> <li>More Flexibility:\u00a0Enables you to set variables as red or write-only. Examples include: setName(), setAge() or to set variables as write-only then you only need to omit the get methods like getName(), getAge() etc.</li> <li>Easy to Reuse:\u00a0With encapsulation it's easy to change and adapt to new requirements.</li> </ul>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#abstraction","title":"Abstraction","text":"<p>Abstraction in Object Oriented Programming solves the issues at the design level.</p> <p>Displays essential information and hides unwanted information, such as the inner details of how objects are implemented, from the user. Abstraction is vital to reduce programming complexity as it allows programmers to display only the relevant information of the object to the user.</p> <p>That enables the user to implement more complex logic on top of the provided abstraction without understanding or even thinking about all the hidden complexity.</p> <p>ex:</p> <pre><code>class Employee{\n    private name;\n    private baseSalary;\n\n    setName(val){\n        this.#name = val;\n    }\n    setBaseSalary(val){\n        this.#baseSalary = val;\n    }\n\n    getName(){\n        return this.#name;\n    }\n\n    getSalary(){\n        let bonus = 1000;\n        return this.#baseSalary + bonus;\n    }\n}\nvar emp = new Employee();\nemp.setName(\"abc\");\nemp.setBaseSalary(100);\nconsole.log(emp.getName());\nconsole.log(emp.getSalary());\n</code></pre>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#inheritance","title":"Inheritance","text":"<p>It is a mechanism where you can to derive a class from another class for a hierarchy of classes that share a set of attributes and methods.</p> <p>This concept lets programmers create new objects with the same attributes as old or existing objects. In essence, we are inheriting the attributes of parent objects to child objects.</p> <p>ex:</p> <pre><code>class Fruit {\n    name\n    color\n    constructor(name, color) {\n        this.name = name\n        this.color = color\n    }\n}\n\n// Strawberry is inherited from Fruit\nclass Strawberry extends Fruit {\n    constructor(name, color) {\n        super(name, color)\n    }\n}\n</code></pre>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#polymorphism","title":"Polymorphism","text":"<p>Allows programmers to run a child object exactly like the parent whilst retaining their specific methods.</p> <p>Let\u2019s take a new example to understand this concept further. Say we have an object called \u201cVehicle\u201d, and we create new child objects, called \u201cCar,\u201d \u201cBus,\u201d and \u201cTrain.\u201d</p> <p>Consider that the vehicle object has a method called \u201crun.\u201d</p> <p>Now according to the principles of Polymorphism, this method can be passed on to the child objects as well, but the way each child object implements this method will be different and specific to them.</p> <p>Polymorphism is the practice to design objects in such a way that they share and override behavior from parent objects.</p>"},{"location":"Computer-Science/OOP-vs-Functional-Programming/#data-type-vs-data-structure","title":"Data Type vs Data Structure","text":"<p>A Data type is one of the forms of a variable to which the value can be assigned of a given type only. This value can be used throughout the program.</p> <p>A\u00a0data type\u00a0is the most basic and the most common classification of data. It is this through which the compiler gets to know the form or the type of information that will be used throughout the code. So basically data type is a type of information transmitted between the programmer and the compiler where the programmer informs the compiler about what type of data is to be stored and also tells how much space it requires in the memory. Some basic examples are int, string etc. It is the type of any variable used in the code.</p> <p>ex:</p> <pre><code>int a = 5;\nfloat b = 5.0;\nchar c = 'A';\n</code></pre> <p>the variable \u2018a\u2019 is of data type integer which is denoted by int a. So the variable \u2018a\u2019 will be used as an integer type variable throughout the process of the code. And, in the same way, the variables \u2018b\u2019 and \u2018c\u2019 are of type float and character respectively. And all these are kinds of data types.</p> <p>A\u00a0data structure\u00a0is a collection of different forms and different types of data that has a set of specific operations that can be performed. It is a collection of data types. It is a way of organizing the items in terms of memory, and also the way of accessing each item through some defined logic. Some examples of data structure are arrays, lists, stacks, queues, linked lists, binary tree and many more.</p> <p>It\u2019s like a container!</p> <p>For example, you have to store data for many employees where each employee has his name, employee id and a mobile number. So this kind of data requires complex data management, which means it requires data structure comprised of multiple primitive data types. So data structures are one of the most important aspects when implementing coding concepts in real-world applications.</p> <p>Data Structure is the collection of different kinds of data. That entire data can be represented using an object and can be used throughout the entire program.</p>"},{"location":"Computer-Science/core-thread-process/","title":"Computer Concepts (Program, Core, Process, Thread,...)","text":""},{"location":"Computer-Science/core-thread-process/#programs","title":"Programs","text":"<p>A program is a code that is stored on your computer that is intended to fulfill a certain task. There are many types of programs, including programs that help your computer function and are part of the operating system, and other programs that fulfill a particular job. These task-specific programs are also known as \u201capplications\u201d and can include programs such as word processing, web browsing,...</p> <p>Programs are typically stored on disk or in non-volatile memory in a form that can be executed by your computer. Prior to that, they are created using a programming language using instructions that involve logic, data and device manipulation, recurrence, and user interaction. The end result is a text file of code that is compiled into binary form (ones and zeros) in order to run on the computer. Another type of program is called interpreted and instead of being compiled in advance in order to run, is interpreted into executable code at the time it is run. Some common, typically interpreted programming languages, are Python, PHP, JavaScript, and Ruby.</p>"},{"location":"Computer-Science/core-thread-process/#process","title":"Process","text":"<p>The program has been loaded into the computer\u2019s memory in binary form. Now what?</p> <p>The program needs memory and various operating system resources in order to run. A \u201cprocess\u201d is what we call a program that has been loaded into memory along with all the resources it needs to operate. The \u201coperating system\u201d is the brains behind allocating all these resources. The OS handles the task of managing the resources needed to turn your program into a running process.</p> <p>There can be multiple instances of a single program, and each instance of that running program is a process. Each process has a separate memory address space, which means that a process runs independently and is isolated from other processes. It cannot directly access shared data in other processes. Switching from one process to another requires some time (relatively) for saving and loading registers, memory maps, and other resources.</p> <p>This independence of processes is valuable because the operating system tries its best to isolate processes so that a problem with one process doesn\u2019t corrupt or cause havoc with another process. You\u2019ve undoubtedly run into the situation in which one application on your computer freezes or has a problem and you\u2019ve been able to quit that program without affecting others.</p>"},{"location":"Computer-Science/core-thread-process/#thread","title":"Thread","text":"<p>Thread is a single sequential flow of control in a program that allows multiple activities within a single process. A single thread is like one command that runs at a time.</p> <p>Most processor manufacturers use the Simultaneous multithreading (SMT) technique to make sure a single processer can run multiple threads. Multithreading is similar to multitasking in which multiple threads are executed at a time, and a multithread ability manages numerous requests by the same user without opening multiple copies of programs running on the computer.</p> <p>A thread is a unit of execution on concurrent/parallel programming. Multithreading is a technique which allows a CPU to execute many tasks of one process at the same time. These threads can execute individually while sharing their resources.</p>"},{"location":"Computer-Science/core-thread-process/#how-threads-work","title":"How Threads Work","text":"<p>A thread is the unit of execution within a process. A process can have from just one thread to many threads.</p> <p></p> <p>When a process starts, it is assigned memory and resources. Each thread in the process shares that memory and resources. In single-threaded processes, the process contains one thread. The process and the thread are one and the same, and there is only one thing happening. </p> <p>In multithreaded processes, the process contains more than one thread, and the process is accomplishing a number of things at the same time</p> <p></p> <p>Each thread will have its own stack.</p> <p>Threads are sometimes called lightweight processes because they have their own stack but can access shared data. Because threads share the same address space as the process and other threads within the process, the operational cost of communication between the threads is low, which is an advantage. The disadvantage is that a problem with one thread in a process will certainly affect other threads and the viability of the process itself.</p> <p>Review:</p> <ol> <li>The program starts out as a text file of programming code.</li> <li>The program is compiled or interpreted into binary form.</li> <li>The program is loaded into memory.</li> <li>The program becomes one or more running processes.</li> <li>Processes are typically independent of each other.</li> <li>Threads exist as the subset of a process.</li> <li>Threads can communicate with each other more easily than processes can.</li> <li>Threads are more vulnerable to problems caused by other threads in the same process.</li> </ol>"},{"location":"Computer-Science/core-thread-process/#thread-vs-process","title":"Thread vs Process","text":"<p>When Google was designing the Chrome browser, they needed to decide how to handle the many different tasks that needed computer, communications, and network resources at the same time. Each browser window or tab communicates with multiple servers on the internet to retrieve text, programs, graphics, audio, video, and other resources, and renders that data for display and interaction with the user. In addition, the browser can open many windows, each with many tasks.</p> <p>Google had to decide how to handle that separation of tasks. They chose to run each browser window in Chrome as a separate process rather than a thread or many threads, as is common with other browsers. Doing that brought Google a number of benefits. Running each window as a process protects the overall application from bugs and glitches in the rendering engine and restricts access from each rendering engine process to others and to the rest of the system. Isolating a JavaScript program in a process prevents it from running away with too much CPU time and memory and making the entire browser non-responsive.</p> <p>Google made a calculated trade-off with the multi-processing design. Starting a new process for each browser window has a higher fixed cost in memory and resources than using threads. They were betting that their approach would end up with less memory bloat overall.</p> <p>Using processes instead of threads also provides better memory usage when memory gets low. An inactive window is treated as a lower priority by the operating system and becomes eligible to be swapped to disk when memory is needed for other processes. That helps keep the user-visible windows more responsive. If the windows were threaded, it would be more difficult to separate the used and unused memory as cleanly, wasting both memory and performance.</p>"},{"location":"Computer-Science/core-thread-process/#how-multithreading-works","title":"How Multithreading Works?","text":"<p>For example, most modern CPUs support multithreading. A simple app on your smartphone can give you a live demo of the same.</p> <p>When you open an app that requires some data to be fetched from the internet, the content area of the app is replaced by a spinner. This will rotates until the data is fetched and displayed.</p> <p>In the background, there are two threads:</p> <ul> <li>One fetching the data from a network, and</li> <li>One rendering the GUI that displays the spinner</li> </ul> <p>Both of these threads execute one after the other to give the illusion of concurrent execution</p>"},{"location":"Computer-Science/core-thread-process/#working-of-core-and-thread","title":"Working of Core and Thread","text":"<p>The core is a hardware component and performs and has the ability to run one task at one time.</p> <p>But multiple cores can support multiple applications to be executed without any disruptions.</p> <p>If the user is planning to set up a game, some parts of cores are required to run the game, some needed to check other background applications like skype, chrome, Facebook, etc. But the CPU should supports multithreading to executes these effectively to fetch the relevant information from the application within a minimum response time. Multithreading just makes the process fast and organized, and convert into better performance. It increases power consumption but rarely causes a rise in temperature.</p> <p>A CPU performance will depend upon the number of cores on the machine and the speed at which the individual cores can execute instructions.</p>"},{"location":"Computer-Science/core-thread-process/#concurrency-and-parallelism","title":"Concurrency and Parallelism","text":"<p>On a system with multiple processors or CPU cores (as is common with modern processors), multiple processes or threads can be executed in parallel. On a single processor, though, it is not possible to have processes or threads truly executing at the same time. In this case, the CPU is shared among running processes or threads using a process scheduling algorithm that divides the CPU\u2019s time and yields the illusion of parallel execution. The time given to each task is called a \u201ctime slice.\u201d The switching back and forth between tasks happens so fast it is usually not perceptible. The terms, \u201cparallelism\u201d (genuine simultaneous execution) and \u201cconcurrency\u201d (interleaving of processes in time to give the appearance of simultaneous execution), distinguish between the two types of real or approximate simultaneous operation.</p> <p></p>"},{"location":"Computer-Science/core-thread-process/#what-is-concurrency-or-single-core","title":"What is Concurrency or Single Core?","text":"<p>a Single-core CPU will only be able to process one program at a time. However, when you run multiple programs simultaneously, then a single-core processor will divide all programs into small pieces and concurrently execute with time slicing, as you can view in the picture given below.</p> <p>Concurrency is defined as the ability of a system to run two or more programs in overlapping time phases.</p> <p></p> <p>Concurrent execution with time slicing</p> <p>As you can see, at any given time, there is only one process in execution. Therefore, concurrency is only a generalized approximation of real parallel execution. This kind of situation can be found in systems having a single-core processor.</p>"},{"location":"Computer-Science/core-thread-process/#what-is-parallel-execution-or-multi-core","title":"What is Parallel Execution or (Multi-Core)?","text":"<p>A Multicore processor (multiple CPU cores) execute each sub-task simultaneously</p> <p>In parallel execution, the tasks to be performed by a process are broken down into sub-parts, and multiple CPUs (or multiple cores) process each sub-task at precisely the same time.</p> <p></p> <p>As you can see, at any given time, all processes are in execution. In reality, it is the sub-tasks of a process which are executing in parallel, but for better understanding, you can visualize them as processes.</p> <p>Therefore, parallelism is the real way in which multiple tasks can be processed at the same time. This type of situation can be found in systems having multicore processors, which includes almost all modern, commercial processors.</p>"},{"location":"Computer-Science/core-thread-process/#key-differences","title":"Key Differences","text":"<p>\u2022 Cores increase the amount of work accomplished at a time, whereas threads improve throughput, computational speed-up.</p> <p>\u2022 Cores is an actual hardware component whereas thread is a virtual component that manages the tasks.</p> <p>\u2022 Cores use content switching while threads use multiple CPUs for operating numerous processes.</p> <p>\u2022 Cores require only a signal process unit whereas threads require multiple processing units.</p>"},{"location":"Computer-Science/core-thread-process/#cpu-core","title":"CPU Core","text":"<p>A CPU core is the part of something central to its existence or character. In the same way in the computer system, the CPU is also referred to as the core.</p> <p>There are basically two types of core processor:</p> <ol> <li>Single-Core Processor</li> <li>Multi-Core Processor</li> </ol>"},{"location":"Computer-Science/core-thread-process/#what-is-the-main-issue-with-single-core","title":"What is the Main Issue with Single Core?","text":"<p>There are mainly two issues with Single Core.</p> <ul> <li>To execute the tasks faster, you need to increase the clock time.</li> <li>Increasing clock time increases power consumption and heat dissipation to an extremely high level, which makes the processor inefficient.</li> </ul>"},{"location":"Computer-Science/core-thread-process/#the-solution-provided-by-multi-core","title":"The Solution Provided by Multi-Core:","text":"<ul> <li>Creating two cores or more on the same die to increase the processing power while it also keeps clock speed at an efficient level.</li> <li>A processor with two cores running an efficient speed can process instructions with similar speed to the single-core processor. Its clock speed is twice, yet the multicore process consumes less energy.</li> </ul>"},{"location":"Computer-Science/core-thread-process/#benefits-of-multi-core-processor","title":"Benefits of Multi-core Processor","text":"<p>Here are some advantages of the multicore processor:</p> <ul> <li>More transistor per choice</li> <li>Shorter connections</li> <li>Lower capacitance</li> <li>A small circuit can work at fast speed</li> </ul>"},{"location":"Computer-Science/core-thread-process/#compiler-vs-interpreter","title":"Compiler vs. Interpreter","text":"<p>Computers can only understand a program written in a binary system known as machine code.</p> <p>To speak to a computer in its non-human language, we came up with two solutions: interpreters and compilers. Ironically, most of us know very little about them, although they belong to our daily coding life.</p> <p>A compiler is a computer program that transforms code written in a high-level programming language into the machine code. It is a program which translates the human-readable code to a language a computer processor understands (binary 1 and 0 bits). The computer processes the machine code to perform the corresponding tasks.</p> <p>An interpreter is a computer program, which converts each high-level program statement into the machine code. This includes source code, pre-compiled code, and scripts. Both compiler and interpreters do the same job which is converting higher level programming language to machine code. However, a compiler will convert the code into machine code (create an exe) before program run. Interpreters convert code into machine code when the program is run.</p> <ul> <li>A compiler translates a code written in a high-level programming language into\u00a0a lower-level language like assembly language, object code, and\u00a0machine code (binary 1 and 0 bits). It converts the code ahead of time before the program runs.</li> <li>An interpreter translates the code line by line when the program is running. You\u2019ve likely used interpreters unknowingly at some point in your work career.</li> </ul> <p></p> <p>Both compilers and interpreters have pros and cons:</p> <ul> <li>Compiler transforms code written in a high-level programming language into the machine code, at once, before program runs, whereas an Interpreter converts each high-level program statement, one by one, into the machine code, during program run.</li> <li>Compiled code runs faster while interpreted code runs slower.</li> <li>Compiler displays all errors after compilation, on the other hand, the Interpreter displays errors of each line one by one.</li> <li>Compiler is based on translation linking-loading model, whereas Interpreter is based on Interpretation Method.</li> <li>Compiler takes an entire program whereas the Interpreter takes a single line of code.</li> </ul> <p></p> <p>A high-level programming language is usually referred to as \u201ccompiled language\u201d or \u201cinterpreted language.\u201d However, in practice, they can have both compiled and interpreted implementations. C, for example, is called a compiled language, despite the existence of C interpreters. The first JavaScript engines were simple interpreters, but all modern engines use just-in-time (JIT) compilation for performance reasons.</p>"},{"location":"Computer-Science/core-thread-process/#refs","title":"Refs","text":"<ul> <li>https://www.backblaze.com/blog/whats-the-diff-programs-processes-and-threads/</li> </ul>"},{"location":"Computer-Science/https-tls-mtls/","title":"TLS, SSL, mTLS, HTTP, HTTPS","text":""},{"location":"Computer-Science/https-tls-mtls/#what-is-tls","title":"What is TLS?","text":"<p>Transport Layer Security, or TLS, is a widely adopted security\u00a0protocol designed to facilitate privacy and data security for communications over the Internet. A primary use case of TLS is encrypting the communication between web applications and servers, such as web browsers loading a website. TLS can also be used to encrypt other communications such as email, messaging, and\u00a0voice over IP (VoIP)</p> <p>TLS, which was formerly called\u00a0SSL, authenticates the server in a\u00a0client-server connection and encrypts communications between client and server so that external parties cannot spy on the communications.</p> <p>There are three important things to understand about how TLS works:</p> <p>1. Public key and private key</p> <p>TLS works using a technique called\u00a0public key cryptography, which relies on a pair of keys \u2014 a public key and a private key. Anything encrypted with the public key can be decrypted only with the\u00a0private\u00a0key.</p> <p>Therefore, a server that decrypts a message that was encrypted with the public key proves that it possesses the private key. Anyone can view the public key by looking at the domain's or server's TLS certificate.</p> <p>2. TLS certificate</p> <p>A TLS certificate is a data file that contains important information for verifying a server's or device's identity, including the public key, a statement of who issued the certificate (TLS certificates are issued by a certificate authority), and the certificate's expiration date.</p> <p>3. TLS handshake</p> <p>The\u00a0TLS handshake\u00a0is the process for verifying the TLS certificate and the server's possession of the private key. The TLS handshake also establishes how encryption will take place once the handshake is finished.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-does-mtls-work","title":"How does mTLS work?","text":"<p>Normally in TLS, the server has a TLS certificate and a public/private key pair, while the client does not. The typical TLS process works like this:</p> <ol> <li>Client connects to server</li> <li>Server presents its TLS certificate</li> <li>Client verifies the server's certificate</li> <li>Client and server exchange information over encrypted TLS connection</li> </ol> <p></p> <p>mTLS is not a different protocol. It is just an extension of the TLS standard.</p> <p>In mTLS, however, both the client and server have a certificate, and both sides authenticate using their public/private key pair. Compared to regular TLS, there are additional steps in mTLS to verify both parties (additional steps in\u00a0bold):</p> <ol> <li>Client connects to server</li> <li>Server presents its TLS certificate</li> <li>Client verifies the server's certificate</li> <li>Client presents its TLS certificate</li> <li>Server verifies the client's certificate</li> <li>Server grants access</li> <li>Client and server exchange information over encrypted TLS connection</li> </ol> <p></p> <p>Certificate authorities in mTLS</p> <p>The organization implementing mTLS acts as its own certificate authority. This contrasts with standard TLS, in which the certificate authority is an external organization that checks if the certificate owner legitimately owns the associated\u00a0domain.</p> <p>A \"root\" TLS certificate is necessary for mTLS; this enables an organization to be their own certificate authority. The certificates used by authorized clients and servers have to correspond to this root certificate. The root certificate is self-signed, meaning that the organization creates it themselves. (This approach does not work for one-way TLS on the public Internet because an external certificate authority has to issue those certificates.)</p>"},{"location":"Computer-Science/https-tls-mtls/#why-use-mtls","title":"Why use mTLS?","text":"<p>mTLS helps ensure that traffic is secure and trusted in both directions between a client and server. This provides an additional layer of security for users who log in to an organization's network or applications. It also verifies connections with client devices that do not follow a login process, such as Internet of Things devices.</p> <p>mTLS prevents various kinds of attacks, including:</p> <ul> <li>On-path attacks:\u00a0On-path attackers\u00a0place themselves between a client and a server and intercept or modify communications between the two. When mTLS is used, on-path attackers cannot authenticate to either the client or the server, making this attack almost impossible to carry out.</li> <li>Spoofing attacks:\u00a0Attackers can attempt to \"spoof\" (imitate) a web server to a user, or vice versa. Spoofing attacks are far more difficult when both sides have to authenticate with TLS certificates.</li> <li>Credential stuffing:\u00a0Attackers use leaked sets of credentials from a\u00a0data breach\u00a0to try to log in as a legitimate user. Without a legitimately issued TLS certificate,\u00a0credential stuffing\u00a0attacks cannot be successful against organizations that use mTLS.</li> <li>Brute force attacks:\u00a0Typically carried out with\u00a0bots, a\u00a0brute force attack\u00a0is when an attacker uses rapid trial and error to guess a user's password. mTLS ensures that a password is not enough to gain access to an organization's network. (Rate limiting\u00a0is another way to deal with this type of bot attack.)</li> <li>Phishing attacks:\u00a0The goal of a phishing attack\u00a0is often to steal user credentials, then use those credentials to compromise a network or an application. Even if a user falls for such an attack, the attacker still needs a TLS certificate and a corresponding private key in order to use those credentials.</li> <li>Malicious API requests:\u00a0When used for\u00a0API security, mTLS ensures that API requests come from legitimate, authenticated users only. This stops attackers from sending malicious API requests that aim to exploit a vulnerability or subvert the way the API is supposed to function.</li> </ul>"},{"location":"Computer-Science/https-tls-mtls/#websites-already-use-tls-so-why-is-mtls-not-used-on-the-entire-internet","title":"Websites already use TLS, so why is mTLS not used on the entire Internet?","text":"<p>For everyday purposes, one-way authentication provides sufficient protection. The goals of TLS on the public Internet are:</p> <ol> <li>to ensure that people do not visit\u00a0spoofed websites.</li> <li>to keep\u00a0private data\u00a0secure and encrypted as it crosses the various networks that\u00a0comprise the Internet.</li> <li>to make sure that data is not altered in transit. One-way TLS, in which the client verifies the server's identity only, accomplishes these goals.</li> </ol> <p>Additionally, distributing TLS certificates to all end user devices would be extremely difficult. Generating, managing, and verifying the billions of certificates necessary for this is a near-impossible task.</p> <p>But on a smaller scale, mTLS is highly useful and quite practical for individual organizations, especially when those organizations employ a Zero Trust approach to network security. Since a Zero Trust approach does not trust any user, device, or request by default, organizations must be able to authenticate every user, device, and request every time they try to access any point in the network. mTLS helps make this possible by authenticating users and verifying devices.</p>"},{"location":"Computer-Science/https-tls-mtls/#what-is-the-difference-between-tls-and-ssl","title":"What is the difference between TLS and SSL?","text":"<p>TLS is the successor of SSL.</p> <p>TLS evolved from a previous encryption protocol called Secure Sockets Layer (SSL), which was developed by Netscape. TLS version 1.0 actually began development as SSL version 3.1, but the name of the protocol was changed before publication in order to indicate that it was no longer associated with Netscape. Because of this history, the terms TLS and SSL are sometimes used interchangeably.</p>"},{"location":"Computer-Science/https-tls-mtls/#what-is-the-difference-between-tls-and-https","title":"What is the difference between TLS and HTTPS?","text":"<p>HTTPS\u00a0is an implementation of TLS encryption on top of the\u00a0HTTP\u00a0protocol, which is used by all websites as well as some other web services. Any website that uses HTTPS is therefore employing TLS encryption.</p>"},{"location":"Computer-Science/https-tls-mtls/#what-is-http","title":"What is HTTP?","text":"<p>The Hypertext Transfer Protocol (HTTP) is the foundation of the World Wide Web, and is used to load web pages using hypertext links. HTTP is an\u00a0application layer\u00a0protocol designed to transfer information between networked devices and runs on top of other layers of the network protocol stack. A typical flow over HTTP involves a client machine making a request to a server, which then sends a response message.</p>"},{"location":"Computer-Science/https-tls-mtls/#whats-in-an-http-request","title":"What\u2019s in an HTTP request?","text":"<p>An HTTP request is the way internet communications platforms such as web browsers ask for the information they need to load a website. Each HTTP request made across the Internet carries with it a series of encoded data that carries different types of information. A typical HTTP request contains:</p> <ol> <li>HTTP version type</li> <li>a URL</li> <li>an HTTP method</li> <li>HTTP request headers</li> <li>Optional HTTP body.</li> </ol>"},{"location":"Computer-Science/https-tls-mtls/#what-is-https","title":"What is HTTPS?","text":"<p>Hypertext transfer protocol secure (HTTPS) is the secure version of\u00a0HTTP, which is the primary protocol used to send data between a web browser and a website. HTTPS is encrypted in order to increase security of data transfer. This is particularly important when users transmit sensitive data, such as by logging into a bank account, email service, or health insurance provider.</p> <p>Any website, especially those that require login credentials, should use HTTPS. In modern web browsers such as Chrome, websites that do not use HTTPS are marked differently than those that are.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-does-https-work","title":"How does HTTPS work?","text":"<p>HTTPS uses an\u00a0encryption\u00a0protocol to encrypt communications. The protocol is called\u00a0Transport Layer Security (TLS), although formerly it was known as\u00a0Secure Sockets Layer (SSL). This protocol secures communications by using what\u2019s known as an\u00a0asymmetric public key infrastructure. This type of security system uses two different keys to encrypt communications between two parties:</p> <ol> <li>The private key - this key is controlled by the owner of a website and it\u2019s kept, as the reader may have speculated, private. This key lives on a web server and is used to decrypt information encrypted by the public key.</li> <li>The public key - this key is available to everyone who wants to interact with the server in a way that\u2019s secure. Information that\u2019s encrypted by the public key can only be decrypted by the private key.</li> </ol>"},{"location":"Computer-Science/https-tls-mtls/#why-is-https-important-what-happens-if-a-website-doesnt-have-https","title":"Why is HTTPS important? What happens if a website doesn\u2019t have HTTPS?","text":"<p>HTTPS prevents websites from having their information broadcast in a way that\u2019s easily viewed by anyone snooping on the network. When information is sent over regular HTTP, the information is broken into packets of data that can be easily \u201csniffed\u201d using free software. This makes communication over the an unsecure medium, such as public Wi-Fi, highly vulnerable to interception. In fact, all communications that occur over HTTP occur in plain text, making them highly accessible to anyone with the correct tools, and vulnerable to\u00a0on-path attacks.</p> <p>With HTTPS, traffic is encrypted such that even if the packets are sniffed or otherwise intercepted, they will come across as nonsensical characters. Let\u2019s look at an example:</p> <p>Before encryption:</p> <pre><code>This is a string of text that is completely readable\n</code></pre> <p>After encryption:</p> <pre><code>ITM0IRyiEhVpa6VnKyExMiEgNveroyWBPlgGyfkflYjDaaFf/Kn3bo3OfghBPDWo6AfSHlNtL8N7ITEwIXc1gU5X73xMsJormzzXlwOyrCs+9XCPk63Y+z0=\n</code></pre> <p>In websites without HTTPS, it is possible for Internet service providers (ISPs) or other intermediaries to inject content into webpages without the approval of the website owner. This commonly takes the form of advertising, where an ISP looking to increase revenue injects paid advertising into the webpages of their customers. Unsurprisingly, when this occurs, the profits for the advertisements and the quality control of those advertisements are in no way shared with the website owner. HTTPS eliminates the ability of unmoderated third parties to inject advertising into web content.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-is-https-different-from-http","title":"How is HTTPS different from HTTP?","text":"<p>Technically speaking, HTTPS is not a separate protocol from HTTP. It is simply using TLS/SSL encryption over the HTTP protocol. HTTPS occurs based upon the transmission of\u00a0TLS/SSL certificates, which verify that a particular provider is who they say they are.</p> <p>When a user connects to a webpage, the webpage will send over its SSL certificate which contains the public key necessary to start the secure session. The two computers, the client and the server, then go through a process called an SSL/TLS handshake, which is a series of back-and-forth communications used to establish a secure connection. To take a deeper dive into encryption and the SSL/TLS handshake.</p>"},{"location":"Computer-Science/https-tls-mtls/#how-does-a-website-start-using-https","title":"How does a website start using HTTPS?","text":"<p>Many website hosting providers and other services will offer TLS/SSL certificates for a fee. These certificates will be often be shared amongst many customers. More expensive certificates are available which can be individually registered to particular web properties.</p>"},{"location":"Computer-Science/https-tls-mtls/#refs","title":"Refs","text":"<ul> <li>Cloudflare</li> </ul>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/","title":"Reactive Programming vs Reactive Systems","text":"<p>Reactive programming \u2014 focusing on computation through ephemeral dataflow chains \u2014 tends to be\u00a0event-driven, while reactive systems \u2014 focusing on resilience and elasticity through the communication, and coordination, of distributed systems\u2014is\u00a0message-driven\u00a0(also referred to as\u00a0messaging).</p> <p>When people talk about \u201creactive\u201d in the context of software development and design, they generally mean one of three things:</p> <ul> <li>Reactive systems (architecture and design)</li> <li>Reactive programming (declarative event-based)</li> <li>Functional reactive programming (FRP)</li> </ul>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#reactive-programming","title":"Reactive Programming","text":"<p>Reactive Programming is an asynchronous programming model where the flow is composed of observable sources and reactions to events without sacrifice.  Is a programming paradigm that deals with asynchronous data streams and the propagation of changes in those streams. It allows you to build reactive and event-driven systems that can easily handle real-time data.</p> <p>Can be divided into two worlds:</p> <ul> <li>Event-loop (I/O Reactor, Netty, AsyncHttp);</li> </ul> <p>The event-loop model is based on the reactor pattern. The main idea behind this pattern is to have a handler (which in Node.js is represented by a callback function) associated with each I/O operation. When an event occurs, it is processed by the event loop, which invokes the appropriate handler to handle the event. In other words, The main idea behind the reactor pattern is to have a\u00a0handler which will be invoked as soon as an event is produced and processed by the event loop.</p> <ul> <li>Reactive-extensions implementation (RxJava, Reactor, Mutiny);</li> </ul> <p>The Reactive Extensions library extends the observer pattern and enables the composition of asynchronous and event-based programs. It lets you treat streams of asynchronous events with the same sort of simple, composable operations that you use for collections of data items, like arrays.</p> <p>Features:</p> <ul> <li>Publisher (Observable source, has a subscribe method)<ul> <li>Multi(0 .. N or error) \u2192 can publish 0, error, or infinite events to its subscribers.</li> <li>Single(0 .. 1 or error) \u2192 can publish 0, error, or 1 event to its subscribers</li> </ul> </li> <li>Lazy evaluation (nothing happens until someone subscribes)</li> <li>Hot vs Cold Publishers \u2192 Hot Publishers do not create new data producers for each new subscription (as the Cold Publisher does). Instead, there will be only one data producer and all the observers listen to the data produced by the single data producer. So all the observers get the same data.</li> <li>Schedulers \u2192 allow you to control the threading and concurrency.</li> </ul>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#the-reactor-pattern","title":"The Reactor Pattern","text":"<p>The reactor pattern is one implementation technique of event-driven architecture. In simple terms, it uses a single-threaded event loop blocking on resource-emitting events and dispatches them to corresponding handlers and callbacks.</p> <p>There is no need to block on I/O, as long as handlers and callbacks for events are registered to take care of them. Events refer to instances like a new incoming connection, ready for read, ready for write, etc.  Those handlers/callbacks may utilize a thread pool in multi-core environments.</p> <p>This pattern decouples the modular application-level code from reusable reactor implementation.</p> <p>There are two important participants in the architecture of Reactor Pattern:</p> <p>1. Reactor</p> <p>A Reactor runs in a separate thread, and its job is to react to IO events by dispatching the work to the appropriate handler. It\u2019s like a telephone operator in a company who answers calls from clients and transfers the line to the appropriate contact.</p> <p>2. Handlers</p> <p>A Handler performs the actual work to be done with an I/O event, similar to the actual officer in the company the client wants to speak to.</p> <p>A reactor responds to I/O events by dispatching the appropriate handler. Handlers perform non-blocking actions.</p>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#asynchronous-programming","title":"Asynchronous Programming","text":"<p>Reactive Programming is a subset of asynchronous programming and a paradigm where the availability of new information drives the logic forward rather than having a control flow driven by a thread of execution.</p> <p>Asynchronous\u00a0is defined by the Oxford Dictionary as \u201cnot existing or occurring at the same time,\u201d which in this context means that the processing of a message or event is happening at some arbitrary time, possibly in the future. This is a very important technique in reactive programming since it allows for\u00a0non-blocking\u00a0execution\u2014where threads of execution competing for a shared resource don\u2019t need to wait by blocking (preventing the thread of execution from performing other work until current work is done), and can as such perform other useful work while the resource is occupied. Amdahl\u2019s Law\u00a0tells us that contention is the biggest enemy of scalability, and therefore a reactive program should rarely, if ever, have to block.</p>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#why-do-we-need-asynchronous-work","title":"Why do we need Asynchronous work?","text":"<p>The simple answer is we want to improve the user experience.\u00a0We want to make our application more responsive. We want to deliver a smooth user experience to our users without freezing the main thread, slowing them down and we don\u2019t want to provide the jenky performance to our users.</p> <p>To keep the main thread free we need to do a lot of heavy and time-consuming work we want to do in the background. We also want to do heavy work and complex calculations on our servers as mobile devices are not very powerful to do the heavy lifting. So we need asynchronous work for network operations.</p> <p>Reactive programming is generally\u00a0event-driven, in contrast to reactive systems, which are\u00a0message-driven</p> <p>The application program interface (API) for reactive programming libraries are generally either:</p> <ul> <li>Callback-based\u2014where anonymous side-effecting callbacks are attached to event sources, and are being invoked when events pass through the dataflow chain.</li> <li>Declarative\u2014through functional composition, usually using well-established combinators like\u00a0map,\u00a0filter,\u00a0fold\u00a0etc.</li> </ul> <p>The primary benefits of reactive programming are: increased utilization of computing resources on multicore and multi-CPU hardware; and increased performance by reducing serialization points and, by extension, Scalability Law.</p> <p>When using reactive programming, data streams are going to be the spine of your application. Events, messages, calls, and even failures are going to be conveyed by a data stream. With reactive programming, you observe these streams and react when a value is emitted.</p> <p>So, in your code, you are going to create data streams of anything and from anything: click events, HTTP requests, ingested messages, availability notifications, changes on a variable, cache events, measures from a sensor, literally anything that may change or happen. This has an interesting side-effect on your application: it\u2019s becoming inherently asynchronous.</p>"},{"location":"Computer-Science/reactive-programming-vs-reactive-systems/#reactive-system","title":"Reactive System","text":"<p>A reactive system\u00a0is an architectural style that allows multiple individual applications to coalesce as a single unit, reacting to its surroundings, while remaining aware of each other\u2014this could manifest as being able to scale up/down, load balancing, and even taking some of these steps proactively. It\u2019s possible to write a single application in a reactive style (i.e. using reactive programming); however, that\u2019s merely one piece of the puzzle. Though each of the above aspects may seem to qualify as \u201creactive,\u201d in and of themselves they do not make a\u00a0system\u00a0reactive.</p> <p>In a Reactive System, it\u2019s the interaction between the individual parts that makes all the difference, which is the ability to operate individually yet act in concert to achieve their intended result.</p> <p>Messages have a clear (single) destination, while events are facts for others to observe. Furthermore, messaging is preferably asynchronous, with the sending and the reception decoupled from the sender and receiver respectively.</p> <p>Messages are needed to communicate across the network and form the basis for communication in distributed systems, while events on the other hand are emitted locally.</p> <p>Reactive Systems are designed to be responsive, resilient, elastic, and message-driven. They are highly scalable and can handle a large number of requests and events in a timely and efficient manner. They are capable of processing a large number of events in parallel without blocking or slowing down the system. Reactive Systems are highly resilient and can recover quickly from failures.</p> <p>Examples/UseCases:</p> <ul> <li> <p>Netflix =&gt; Netflix uses a microservices architecture and is built using technologies like Kafka, RxJava, Hystrix, Eureka...</p> </li> <li> <p>Real-time Analytics</p> </li> <li> <p>Internet of Things (IoT)</p> </li> </ul>"},{"location":"GitOps/","title":"GitOps","text":""},{"location":"GitOps/#traditional-approach","title":"Traditional Approach","text":"<p>Before dive into what GitOps is, it's important to understand how applications deployment was done, the traditional approachs..., how developers used to rollout applications version, and how they used to create infrastructure components, this way, it's easier to undestand the problems that the GitOps approach solves.</p> <p>Traditional approach</p> <p>Back then, applications were deployed using imperative scripts or manual processes, developers would roll out new versions of their applications by logging into servers and running scripts or commands to manually update the code and dependencies.</p> <p>for example, a developer that is working on a project feature will merge the changes in the main branch, ssh the virtual machine that is hosting that service, execute the commands to manually pull the changes, install dependencies, and re-run the service</p> <p>In nodejs for example:</p> <pre><code>ssh -i \"key.pem\" ec2-user@ec2-ip.compute-1.amazonaws.com\n\ngit clone ...\n\nnpm install\n\nnpm start\n</code></pre> <p>Best case scenario, this approach will have a pipeline defined to execute the imperative scripts, but there are a lot of developers that still do this manually.</p> <p>This way you have the application running with the new version.</p> <p>This approach has a lot of problems:</p> <ul> <li>time-consuming;</li> <li>error-prone;</li> <li>developers can easily make mistakes or overlook steps when running scripts manually, leading to issues and downtime. </li> <li>hard to track changes and ensure consistency across environments.</li> <li>difficult to rollback changes in case something happen.</li> <li>hard to know the current state of your infrastructure.</li> <li>...</li> </ul> <p>Despite these challenges, some developers and companies still use the old approach today. Perhaps they lack the knowledge or resources to adopt a new methodology, or they simply don't see the value in changing their ways. But the reality is that this approach is not reliable and can hold back innovation and development speed because this approach is script-based, the system configuration is tipically based in imperative scripts, in which is a sequence of steps to setup the system and reach a desired state</p> <p>I think you undesrstood the number of problems with this approach so far, needless to say that you have to create all infra components manually and if you're working with multiple environments you have to do this operation for each env (dev, stage, prod...)</p> <p>And because of these problems, using an declarative approach is extremely important to a modern software development to companies of all sizes</p>"},{"location":"GitOps/#gitops_1","title":"GitOps","text":"<p>In a few words, GitOps is infrastructure as Code (IaC), but it goes far beyond that.</p> <p>GitOps is an evolution of IaC with a recommended DevOps practice that utilize Git as the only source of truth and a pipeline to create/update/destroy the infrastructure, application or system architecture.</p> <p>GitOps leverages Git as a single source of truth for both application code and infrastructure configuration, making it easier to manage and automate the deployment process. With GitOps, developers can define their desired state for an environment in code and use Git to manage changes to that code over time. This ensures that infrastructure changes are tracked, versioned, and auditable, and makes it easier to roll back changes if needed.</p> <p>There are a lot of tools that allow us to create IaC such as Terraform, Ansible, Serverless Framework, CloudFormation, Crossplane, and others. However, to fully adopt GitOps, it's important to use these tools in conjunction with a Git-based deployment pipeline.</p> <p>Remember, Is not only by using one of these IaC tools that you're adopting GitOps, if you're still running local commands to apply these changes then there is a gap in there and you can't rely on git to know your system state.</p> <p>For example, let's say you're using Terraform to manage your infrastructure. With GitOps, you would define your infrastructure as code in Terraform, and store it in a Git repository. You would also define a deployment pipeline that automatically updates your infrastructure based on changes to the code in that repository.</p> <p>You might also use a pull-request strategy so that once the changes are approved and merged into the main branch, the pipeline would automatically deploy them to production.</p> <p>By using this approach, you can ensure that your infrastructure changes are tracked and versioned in Git, and that your deployment process is automated and auditable. This makes it easier to manage your infrastructure at scale.</p> <p>Benefits</p> <ul> <li>declarative approach (follows a declaration of an expected state rather than a sequence of commands.), easier to know and understand what's in your environment</li> <li>reduce errors and downtime</li> <li>enable more rapid innovation and experimentation</li> <li>consistent state</li> </ul>"},{"location":"GitOps/#gitops-approachs","title":"GitOps Approachs","text":"<ul> <li> <p>push-based</p> </li> <li> <p>pull-based</p> </li> </ul>"},{"location":"Kafka/","title":"Apache Kafka","text":"<p>Open source project created by Apache foundation</p> <p>Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. </p> <p>Nowadays everything is based on events, and we have to get these events to understand what happen and deal with the data in the most properly way.</p> <p>Questions</p> <ul> <li>Where do I save these events?</li> <li>How can I restore in a fast and simple way these events so that the feedback between one process and another can happen in a fluidly way and in real time?</li> <li>How to scale?</li> <li>How can I have resilience and high availability?</li> </ul> <p>Kafka \"Super Powers\"</p> <ul> <li>High throughput. can handle thousands of events per second;</li> <li>Low latency (2ms);</li> <li>Scalable;</li> <li>High Storage;</li> <li>High availability;</li> <li>It can connect with almost everything (Go, Node, Python,\u2026);</li> <li>Has a lot of libraries that makes easy to work with Kafka;</li> <li>Open source;</li> </ul>"},{"location":"Kafka/#how-it-works","title":"How it works","text":"<pre><code>                                          [Broker A]\n[Producer] -- Data --&gt; [Apache Kafka] --&gt; [Broker B] &lt;-- data -- [Consumer]\n                                          [Broker C]\n</code></pre> <p>Kafka is a cluster, and this cluster is formed by a set of nodes and these nodes are our brokers;</p> <p>Producer \u21d2 Publish messages;</p> <p>Consumer \u21d2 Consume the message, will access the broker to read the messages;</p> <p>Broker \u21d2 Each broker has its own database, each broker is a machine, and the node responsability is just to store the data;</p> <p>Kafka doesn\u2019t send message to anyone, Kafka just stores the message and the consumer read/retrieve the data;</p> <p>Usually when you deploy a cluster the recommendation is to have at least 3 brokers;</p> <p>These brokers communicate all the time and in order to manage this communication and to see what nodes are available or not the Kafka has another service called zookeper that can work as a service discovery, load balancer and so on\u2026</p>"},{"location":"Kafka/#managed-services","title":"Managed Services","text":"<ul> <li>Confluent Cloud \u2192 the most complete enviroment (Expensive)</li> <li>Amazon MSK (Managed Streaming for Apache Kafka) \u2192 (Expensive) Lowest price = $2.5/day</li> <li>Aiven Kafka</li> <li>...</li> </ul>"},{"location":"Kafka/avro/","title":"Avro","text":"<p>Avro is defined by a schema (schema is written in JSON)</p> <p>Advantages:</p> <ul> <li>Data is fully typed</li> <li>Data is compressed automatically(less CPU usage)</li> <li>Schema (defined using JSON) comes along with the data</li> <li>Documentation is embedded in the schema</li> </ul> <p>Primitive types:</p> <ul> <li>null: no value</li> <li>boolean: binary value</li> <li>int: 32-bit signed integer</li> <li>long: 64-bit signed integer</li> <li>float: single precision(32-bit) IEEE 754 floating-point number</li> <li>double: double precision (64-bit) IEEE 754 floating point number</li> <li>bytes: sequence of 8-bit unsigned bytes</li> <li>string: unicode character sequence</li> </ul>"},{"location":"Kafka/avro/#avro-record-schemas","title":"Avro Record Schemas","text":"<p>Avro Record Schemas are defined using JSON</p> <p>it has some common fields:</p> <ul> <li>Name: name of your schema</li> <li>Namespace: \"package\u201d</li> <li>Doc: documentation to explain your schema</li> <li>Aliases: Optional other names for your schema</li> <li>Fields<ul> <li>Name: field name</li> <li>Doc: field doc</li> <li>Type: field value</li> <li>Default: default value</li> </ul> </li> </ul> <p>ex:</p> <pre><code>{\n    \"type\": \"record\",\n    \"namespace\": \"com.example\",\n    \"name\": \"Customer\",\n    \"doc\": \"Avro schema for our customer\",\n    \"fields\": [\n        {\"name\": \"first_name\", \"type\": \"string\", \"doc\": \"\"},\n    ]\n}\n</code></pre> <p>Others types:</p> <ul> <li>Enums</li> </ul> <pre><code>{\"type\": \"enum\", \"name\": \"CustomerStatus\", \"symbols\": [\"BRONZE\", \"SILVER\", \"GOLD\"]}\n</code></pre> <ul> <li>Array</li> </ul> <pre><code>{\"type\": \"array\", \"items\": [\"string\"]}\n</code></pre> <ul> <li>Maps: Key values</li> </ul> <pre><code>{\"type\": \"map\", \"values\": \"string\"}\n</code></pre> <ul> <li>Unions: allow a field value to take different types</li> </ul> <pre><code>{\"name\": \"middle_name\", \"type\": [\"null\", \"string\"], \"default\": null}\n</code></pre>"},{"location":"Kafka/kafka-connect/","title":"Kafka Connect","text":"<p>Open source platform that works as a data hub to create simple centralizations such as database, key-value stores, search indexes and file systems.</p> <p>Kafka Connect is a framework for connecting Kafka with external systems, including databases.</p> <p>A Kafka Connect cluster is a separate cluster from the Kafka cluster. The Kafka Connect cluster supports running and scaling out connectors (components that support reading and/or writing between external systems).</p> <p>The Kafka connector is designed to run in a Kafka Connect cluster to read data from Kafka topics and write the data into Snowflake tables.</p> <p>How it works?</p> <p>Lets suppose I have my data stored on an CRM such as Salesforce and I want to store that data on another system or a database (Postgres, MySQL,\u2026)</p> <p>I can connect my Kafka Connect on Salesforce, the salesforce data will be stored on a Kafka topic and from this topic I can save on my database;</p> <p>Or if I want to put my SQL data on Mongo, it requires some effort to create this integration and Kafka Connect helps us with this problem without us having to write a line of code</p> <pre><code>[Apache Kafka] &lt;----&gt; [Kafka Connect]\n           MySql   &lt;--- [Connector]\n           MongoDb &lt;--- [Connector]\n                        [Connector] ---&gt; Lambda\n                        [Connector] ---&gt; Elasticsearch\n</code></pre> <p>A connector can be a mysql, mongo, lambda, elastic search, salesforce,\u2026</p> <p>Connectors Type:</p> <ul> <li>Data Sources \u21d2 Get data from somewhere to send to Apache Kafka, Ex:<ul> <li>MySQL</li> <li>Mongo</li> <li>SalesForce</li> </ul> </li> <li>Sinks \u21d2 Where to send this information<ul> <li>Elasticsearch</li> <li>AWS Lamda</li> </ul> </li> </ul> <p>Ex:</p> <ul> <li>I can send MySQL data to a AWS Lambda,</li> <li>Get data from MongoDB and send to a ElasticSearch,</li> <li>\u2026</li> </ul> <p>https://www.confluent.io/hub/</p>"},{"location":"Kafka/kafka-connect/#converters","title":"Converters","text":"<p>The tasks use \u201cconverters\u201d to change the data format for read or write purpose;</p> <p>Popular formats</p> <ul> <li>Avro; better undersantd of json files</li> <li>Protobuf;</li> <li>JsonSchema;</li> <li>Json;</li> <li>String;</li> <li>ByteArray;</li> </ul>"},{"location":"Kafka/kafka-connect/#dlq-dead-letter-queue","title":"DLQ - Dead Letter Queue","text":"<p>When there is an invalid record, for any reason, this error can be handled in the connector configuration through the \u201cerrors.tolerance\u201d property. This type of configuration can only be used for \u201cSink\u201d connectors;</p> <ul> <li>none: the task stops immediately</li> <li>all: errors are ignored and the process continue normally</li> <li>errors.deadletterqueue.topic.name = </li> </ul>"},{"location":"Kafka/producers-and-consumers/","title":"Producers and Consumers","text":""},{"location":"Kafka/producers-and-consumers/#producer","title":"Producer","text":"<p>Responsible for producing messages for a specific topic.</p>"},{"location":"Kafka/producers-and-consumers/#delivery-guarantee","title":"Delivery Guarantee","text":"<pre><code>[Producer] ----&gt; [Broker A][Topic A] Leader\n                 [Broker B][Topic A] Follower\n                 [Broker C][Topic A] Follwer\n</code></pre> <p>The Producer will alwasy send the message to the leader broker first.</p> <p>Acks: The acks setting specifies acknowledgements that the producer requires the leader to receive before considering a request complete.</p> <ul> <li>[Ack 0, None] \u2192 Acknowledge 0, None, AKA FF (Fire and Forget)<ul> <li>It\u2019s the fastest way to send messages because Kafka doesn\u2019t waste time responding to the user so it can handle way more transactions</li> <li>Uber usecase: The driver send his location every 10 seconds, this location is sent to Kafka to handle that message, if Kafka loses 2 location moments, that wouldn't be a drastic loss for the software, so Uber can afford to lose some data in this case;;</li> </ul> </li> <li>[Ack 1, Leader] \u2192 Acknowledge 1, The moment the leader saves the message it will return to the user saying that the message was stored;<ul> <li>The speed is a little bit slower</li> <li>Potencial Problem: the Broker A saved the message and then returned to the users saying that the message was stored but in the same moment the node goes down, and the Broker A didn\u2019t have time to replicate the data to the followers;</li> </ul> </li> <li>[Ack -1, All] \u2192 Acknowledge -1, The producer will send the message to the leader, the leader will replicate to the followers, the followers will notify the leader saying the message was stored and then the leader will respond the client saying the message was safely saved<ul> <li>If you can\u2019t afford lose a message no matter what, you should use this type</li> <li>If Broker A goes down doesn\u2019t matter because the message is stored in other brokers;</li> <li>We\u2019ll lost speed to process the messages</li> </ul> </li> </ul>"},{"location":"Kafka/producers-and-consumers/#producer-usecase-idempotent","title":"Producer Usecase: Idempotent","text":"<p>Suppose you have an application that sends customer orders to a Kafka topic. Each order message has a unique identifier. The consumer application that reads the order messages from the topic processes each order exactly once.</p> <p>Now, suppose the producer application encounters a network error while sending an order message to Kafka. As a result, it retries sending the message, and the message is delivered twice to the Kafka topic. The consumer application will then process the same order twice, resulting in inconsistencies in the order processing.</p> <p>However, if the producer application is configured to use the producer idempotent feature, it will ensure that only one copy of the message is delivered to the Kafka topic, even if multiple retries are attempted. This ensures that the consumer application processes each order exactly once, regardless of any duplicate messages in the Kafka topic.</p> <p></p> <p>The message 4 first got an error but after the retry rule it tried again and was successfully stored, but on the first error the producer tried to send the message again and in this case we will have a duplicated data.</p> <p>Kafka has a way to verify when this happens and if we activate this solution it will cause more slowdown in the system, but is a way to avoid duplicated message and order the messages.</p> <p>The producer idempotent feature ensures that even if a producer sends duplicate messages due to network errors or other issues, Kafka will only store and deliver each message once. This helps prevent data duplication and inconsistencies in the event stream.</p> <p>While the producer idempotent feature in Kafka can be very useful in preventing data duplication and ensuring exactly-once message delivery, there are some potential downsides to consider:</p> <ol> <li> <p>Increased latency: The producer idempotent feature requires additional network round trips between the producer and the Kafka broker to ensure that each message is delivered exactly once. This can increase the overall latency of message production.</p> </li> <li> <p>Higher resource utilization: This feature requires additional memory and CPU resources on both the producer and the broker side to maintain and track the message state.</p> </li> </ol>"},{"location":"Kafka/producers-and-consumers/#consumer","title":"Consumer","text":"<p>Responsible for reading the messages that the producer puts on a topic.</p>"},{"location":"Kafka/producers-and-consumers/#consumers-groups","title":"Consumers Groups","text":"<p>The primary role of a Kafka consumer is to read data from an appropriate Kafka broker. A consumer group is a group of consumers that share the same group id. When a topic is consumed by consumers in the same group, every record will be delivered to only one consumer.</p> <p>If all the consumer instances have the same consumer group, then the records will effectively be load-balanced over the consumer instances</p> <p>This way you can ensure parallel processing of records from a topic and be sure that your consumers won\u2019t be stepping on each other toes.</p> <p>First scenario:</p> <p>One consumer read all partitions</p> <pre><code>[Producer] ---&gt; [    Topic    ]\n                [ Partition 0 ]  ---&gt; [Consumer A]\n                [ Partition 1 ]  ---&gt; [Consumer A]\n                [ Partition 2 ]  ---&gt; [Consumer A]\n</code></pre> <p>Second scenario: Consumer Groups</p> <p>When these consumers are inside a group</p> <pre><code>[Producer] ---&gt; [   Topic     ]       [    Group X   ]\n                [ Partition 0 ]  ---&gt; [  Consumer A  ]\n                [ Partition 1 ]  ---&gt; [  Consumer A  ]\n                [ Partition 2 ]  ---&gt; [  Consumer B  ]\n</code></pre> <p>Consumer A and B can be the same software but running on different machines and because they are the same software and process the same transactions we can put them on a group and the data will be distributed across this group. The Partition 0 and 1 will be read by consumer A and the partition 2 by consumer B.</p> <p>best scenario: 3 partition, 3 consumers</p> <pre><code>[Producer] ---&gt; [   Topic     ]       [    Group X   ]\n                [ Partition 0 ]  ---&gt; [  Consumer A  ]\n                [ Partition 1 ]  ---&gt; [  Consumer B  ]\n                [ Partition 2 ]  ---&gt; [  Consumer C  ]\n</code></pre> <p>usecase:</p> <pre><code>[Producer] ---&gt; [   Topic     ]       [    Group X   ]\n                [ Partition 0 ]  ---&gt; [  Consumer A  ]\n                [ Partition 1 ]  ---&gt; [  Consumer B  ]\n                [ Partition 2 ]  ---&gt; [  Consumer C  ]\n                                      [  Consumer D  ]\n</code></pre> <p>in this case, Consumer D will be AFK because consumers inside a group  cannot read the same data from another consumer.</p> <p>If the consumer doesn\u2019t have a group the consumer itself will be a standalone group and would read all the partitions.</p>"},{"location":"Kafka/topics/","title":"Topics","text":"<p>Topic is a communication channel responsable for receive and make the kafka messages available.</p> <p>If you want to send a message you have to send a message to a topic, the same thing for reading</p> <pre><code>                    /-- [Consumer]\n[Producer] --&gt; Topic &lt;-- [Consumer]\n                    \\-- [Consumer]\n</code></pre> <p>Kafka is different from RabbitMQ because the same message can be read for differents consumers;</p> <p>RabbitMQ save the data in memory while Kafka saves on disk, that way you can read the same message over and over again.</p> <p>Partitions - Anatomy of a record</p> <p>Record:</p> <pre><code>              [ Headers   ]\n[Offset 0] -&gt; | Key       |\n              | Value     |\n              [ Timestamp ]\n</code></pre> <ul> <li>Headers \u2192 Metadata that can be useful for us;</li> <li>Key \u2192 To ensure the delivery order;</li> <li>Value \u2192 Payload, message content;</li> <li>Timestamp \u2192 Created at;</li> </ul> <p>Each topic can have one or more partitions to ensure distribution and resilience of the data;</p> <p>You can think of a partition as being a drawer, it\u2019s a space on the disk where Kafka will store the message. </p> <p>Ex:</p> <pre><code>        ---&gt; [Partition 1] Broker A\nTopic X ---&gt; [Partition 2] Broker A or Broken B\n        ---&gt; [Partition 3] Broker A, B, C or D....\n</code></pre> <p>You cannot have all the eggs on the same basket;</p> <p>The idea is to not have all the messages on the same partition/broker, every time we increase the amount of partition the messages will be more distributed/separeted; If the broker A goes down at least we can have the message on the broker B, C,\u2026</p> <p>Lets suppose we have 1 million messages and single computer, it will require a lot of computational power from this computer and also a lot of effort to process every message; So we can create another computer and to ensure we will not have the both computers reading the same data we split these datas on differents partitions, each computer reads the same topic but from different partitions;</p> <p>Now we have twice more speed/power;</p> <p>Partitions and Keys</p> <p>How can we guarantee the order of messages?</p> <p>About the \u201cKeys\u201d</p> <pre><code>[Partition 1] &lt;---- Consumer 1 (slow)\n        [Offset 0], [Offset 1]\n\n[Partition 2] &lt;---- Consumer 2 (fast)\n        [Offset 0],\n\n[Partition 3] &lt;---- Consumer 3\n        [Offset 0],...\n</code></pre> <p>The only way we can guarantee the order of the messages is when they are on the same partition;</p> <p>Ex: User A buy a product, then at the same moment he will request a refund, but these messages can be in different partitions, the purchase request can be at partition 1 and the refund at the partition 2, but in this scenario the Consumer 1 is slow, so what if this first message receive an error and consumer 2, which is fast, has already processed the refund transaction?</p> <p>In order to guarantee the order of the messages to be executed these messages must be at the same partition and we can do it by using key.</p> <p>Ex:</p> <pre><code>Transfer Message [0] -&gt; Key=Movimentation\nRefund Message   [1] -&gt; Key=Movimentation\nRandom Message   [2] -&gt; No keys\n</code></pre> <p>In this case the first and the second message will be placed on the same partition and the last will be placed with the kafka default behavior, distributing between partitions;</p> <p>Distributed Partitions</p> <p>What usually happens:</p> <pre><code>               ---&gt; [Broker A][Partition 1]\nTopic: [Sales] ---&gt; [Broker B][Partition 2]\n               ---&gt; [Broker C][Partition 3]\n</code></pre> <p>With Replication Factor:</p> <pre><code>               ---&gt; [Broker A][Partition 1][Partition 3]\nTopic: [Sales] ---&gt; [Broker B][Partition 2][Partition 1]\n               ---&gt; [Broker C][Partition 3][Partition 2]\n</code></pre> <p>Replication Factor = 2</p> <p>Replication factor is a way to guarantee data resilience because if the Broker A goes down we have a copy of the partition 1 and 3 on broker B and C. So the most critical our data is, we can have more replication factor to ensure that we will never lose that data;</p> <p>The more replication we have more disk space will be required.</p> <p>The recommendation is to have 2 replication factor and if the application is very critical you can have 3.</p> <p>Partition Leadership</p> <ul> <li>Leaders = Bold</li> <li>Followers = normal</li> </ul> <p>All partitions are on the same topic. ex: Sale</p> [Broker A ] [Broker B ] [Broker C ] [Broker D ] [Partition 1] [Partition 1] [Partition 4] [Partition 3] [Partition 2] [Partition 2] [Partition 2] [Partition 2] [Partition 4] [Partition 3] [Partition 3] [Partition 4] <p>The bold paritions are the leaders, when a consumer has to read a data he will always go to the leader to retrieve that information, even if you have 10 copy of this partition the consumer always is going to the leader.</p> <p>In case a leader partition goes down, the consumer will read from the next available partition, ex: Broker A goes down and is no longer available, so now Broker B will have Partition 1 and 2 as leader. The follower is just a backup in case a leader goes down.</p>"},{"location":"Observability/","title":"Observability","text":"<p>Observability lets us understand a system from the outside, by letting us ask questions about that system without knowing its inner workings. Furthermore, allows us to easily troubleshoot and deal with new problems, and helps us answer the question, \u201cWhy is this happening?\u201d</p> <p>In order to be able to ask those questions of a system, the application must be properly instrumented. That is, the application code must emit\u00a0signals\u00a0such as\u00a0traces,\u00a0metrics, and\u00a0logs. An application is properly instrumented when developers don\u2019t need to add more instrumentation to troubleshoot an issue because they have all of the information they need.</p> <p>It's important to ensure the application is running properly, in a big system we cannot work without observability.</p> <p>\"Imagine you are the pilot of an airplane with 300 people flying, you need the necessary measures (high, velocity,...) to know what decision to take or if a problem happen you need to know where to fix. Our applications is an airplane\"</p> <p>Who doesn't measure doesn't manage;</p> <p>It\u2019s essential to understand when something goes wrong along the application delivery chain so you can identify the root cause and correct it before it impacts your business. Monitoring and observability provide a two-pronged approach. Monitoring supplies situational awareness, and observability helps pinpoint what\u2019s happening and what to do about it.</p> <p>Good monitoring is a staple of high-performing teams. DevOps Research and Assessment (DORA) research shows that a comprehensive monitoring and observability solution, along with a number of other technical practices, positively contributes to continuous delivery.</p>"},{"location":"Observability/#observability-vs-monitoring","title":"Observability vs Monitoring","text":"<p>Observability and monitoring are two related concepts, monitoring is a subset of observability.</p> <p>Monitoring is the process of collecting and analyzing data from a system to assess its health and performance. It involves setting up a predefined set of metrics and logs to track, typically focusing on metrics that are essential for the proper functioning of the system. </p> <p>Observability provides a broader understanding of a system. It involves collecting and analyzing a wide range of data, including metrics, logs, traces, and events to explore properties and patterns not defined in advance.</p>"},{"location":"Observability/#observability_1","title":"Observability","text":"<p>Is tooling or a technical solution that allows teams to actively debug their system. Observability is based on exploring properties and patterns not defined in advance.</p> <p>Observability is the ability to understand a complex system\u2019s internal state by analyzing the data it generates, such as logs, metrics, and traces. it helps teams analyze what\u2019s happening and identify the root cause of a performance problem so they can detect and resolve the underlying causes of issues.</p> <ul> <li>How well you can understand your complex system.</li> <li>It\u2019s a measure of our internal system, how we can get the data output and understand what is happening inside the system.</li> <li>Understand why is wrong, and how that happen?</li> <li>Gain visibility into aspects of the system that were previously unknown or unexpected.</li> <li>Get insight to identify and resolve issues more quickly.</li> <li>Understand how different components are interacting with each other and identify potential points of failure.</li> <li>Enable teams to proactively optimize their systems, by identifying areas for improvement and make data-driven decisions.</li> </ul>"},{"location":"Observability/#monitoring","title":"Monitoring","text":"<p>Monitoring is the task of assessing the health of a system by collecting and analyzing aggregate data based on a predefined set of metrics and logs.</p> <p>It measures the health of the application, such as creating a rule that alerts when the app is nearing 100% disk usage, helping prevent downtime. It shows you not only how the app is functioning, but also how it\u2019s being used over time.</p> <ul> <li>Show when there\u2019s something wrong;</li> <li>Know in advance the signals you want to monitor;</li> <li>Enables teams to proactively detect and diagnose issues before they impact end-users;</li> <li>Detect and diagnose issues before they escalate into more significant problems.</li> <li>Generate alerts when predefined thresholds are crossed or when other predefined conditions are met.</li> </ul> <p></p> <p>For example, in a e-commerce scenario, monitoring would focus on specific metrics that are critical for the proper functioning of each service, such as response time, error rates, and CPU usage. A tool (such as Prometheus) could be used to track these metrics and generate alerts when predefined thresholds are crossed.</p> <p>And the observability would allow teams to explore data in real-time to gain insights into the behavior and performance of the entire system. such as trace requests across the entire system and identify the root cause of the issue.</p> <p>3 Pillars of Observability:</p> <ul> <li> <p>Metrics \u21d2 number, we have 2 types of metrics, business metrics and technical metrics;     technical metrics: CPU: 90%, Memory: 50%...</p> <p>business metrics: 50 new students, 10 students left this month,...</p> </li> <li> <p>Logs \u21d2 result of a specific events;</p> </li> <li> <p>Tracing \u21d2 order of how the event was generated, stacktrace;</p> </li> </ul>"},{"location":"Observability/#tools","title":"Tools","text":"<ul> <li>Elastic Stack</li> <li>Prometheus &amp; Grafana</li> <li>OpenTelemetry</li> <li>Datadog</li> </ul>"},{"location":"Observability/elastic-stack/","title":"Elastic Stack","text":"<p>ELK Stack</p> <ul> <li>Elasticsearch: it can search data more efficiently and scalable; it can do geospatial analysis and visualization; Logging and analytics;<ul> <li>Search engine and analytics;</li> </ul> </li> <li>Logstash: real time data collector engine. get data from many places and transform and send data to many places. Also we can use some plugins to manipulate the data;<ul> <li>data processor through pipeline that can receive, transform and send simultaneous data;</li> </ul> </li> <li>Kibana: Dashboard to visualize and explore the data, is used to: logs, analyse, application monitoring, operational intelligence. Integrated with Elasticsearch, also allow us to aggregate and filter data;<ul> <li>Allow users to visualize the elasticsearch data in different perspective;</li> <li>maps, interactive graphs, dashboards;</li> </ul> </li> </ul>"},{"location":"Observability/elastic-stack/#elk-vs-elastic-stack","title":"ELK vs Elastic Stack ?","text":"<p>ELK \u21d2 Logstash \u2192 Elasticsearch \u2192 Kibana</p> <p>Elastic Stack \u21d2 (Beats, Logstash) \u2192 Elasticsearch \u2192 Kibana</p> <p>Beats \u2192 \u201clightweight data shipper\u201d delivery data on a light way. Data collector agent.</p> <ul> <li>Easy integration with Elasticsearch or Logstash,</li> <li>Get logs, metrics, network data, audit data, uptime monitoring</li> </ul>"},{"location":"Observability/elastic-stack/#elastic","title":"Elastic","text":"<p>Elastic is a company behind the elasticstack, we can use the stack without pay anything but there is some plugins that we need to pay in order to use;</p> <p>Products:</p> <ul> <li>APM \u2192 Application Performance Monitoring</li> <li>Maps</li> <li>Site Search</li> <li>Enterprise search</li> <li>App Search</li> <li>Infrastructure</li> </ul>"},{"location":"Observability/elastic-stack/#kibana","title":"Kibana","text":"<p>Kibana is your window into the Elastic Stack. Specifically, it's a browser-based analytics and search dashboard for Elasticsearch.</p>"},{"location":"Observability/elastic-stack/#projects","title":"Projects","text":"<ul> <li>Observability-ElasticStack</li> </ul>"},{"location":"Observability/open-telemetry/","title":"OpenTelemetry","text":""},{"location":"Observability/open-telemetry/#telemetry-reliability-and-metrics","title":"Telemetry, Reliability and Metrics","text":"<p>Telemetry\u00a0refers to data emitted from a system about its behavior. The data can come in the form of\u00a0Traces,\u00a0Metrics, and\u00a0Logs.</p> <p>Reliability\u00a0answers the question: \u201cIs the service doing what users expect it to be doing?\u201d A system could be up 100% of the time, but if, when a user clicks \u201cAdd to Cart\u201d to add a black pair of pants to their shopping cart, and instead, the system keeps adding a red pair of pants, then the system would be said to be\u00a0unreliable.</p> <p>Metrics\u00a0are aggregations over a period of time of numeric data about your infrastructure or application. Examples include system error rate, CPU utilization, request rate for a given service, and so on\u2026.</p>"},{"location":"Observability/open-telemetry/#logs","title":"Logs","text":"<p>A\u00a0Log\u00a0is a timestamped message emitted by services or other components. Unlike\u00a0Traces, however, they are not necessarily associated with any particular user request or transaction. They are found almost everywhere in software, and have been heavily relied on in the past by both developers and operators alike to help them understand system behavior.</p> <p>Unfortunately, logs aren\u2019t extremely useful for tracking code execution, as they typically lack contextual information, such as where they were called from.</p> <p>They become far more useful when they are included as part of a\u00a0Span.</p>"},{"location":"Observability/open-telemetry/#spans","title":"Spans","text":"<p>A\u00a0Span\u00a0represents a unit of work or operation. It tracks specific operations that a request makes, painting a picture of what happened when that operation was executed.</p> <p>A Span contains name, time-related data,\u00a0structured log messages, and\u00a0other metadata (i.e. Attributes)\u00a0to provide information about the operation it tracks.</p> <p>Below is a sample of the type of information that would be present in a Span:</p> <p>Span Attributes</p> Key Value net.transport IP.TCP net.peer.ip 10.244.0.1 net.peer.port 10243 net.host.name localhost http.method GET http.target /cart http.server_name frontend http.route /cart http.scheme http http.host localhost http.flavor 1.1 http.status_code 200"},{"location":"Observability/open-telemetry/#distributed-traces","title":"Distributed Traces","text":"<p>A\u00a0Distributed Trace, more commonly known as a\u00a0Trace, records the paths taken by requests (made by an application or end-user) as they propagate through multi-service architectures, like microservice and serverless applications.</p> <p>Without tracing, it's challenging to identify the cause of performance issues in a distributed system.</p> <p>It improves the visibility of our application or system\u2019s health and lets us debug behavior that is difficult to reproduce locally. Tracing is essential for distributed systems, which commonly have nondeterministic problems or are too complicated to reproduce locally.</p> <p>Tracing makes debugging and understanding distributed systems less daunting by breaking down what happens within a request as it flows through a distributed system.</p> <p>A Trace is made of one or more Spans. The first Span represents the Root Span. Each Root Span represents a request from start to finish. The Spans underneath the parent provide a more in-depth context of what occurs during a request (or what steps make up a request).</p> <p>Many Observability back-ends visualize Traces as waterfall diagrams that may look something like this:</p> <p></p> <p>Waterfall diagrams show the parent-child relationship between a Root Span and its child Spans. When a Span encapsulates another Span, this also represents a nested relationship.</p> <ul> <li>If you have 5 microservices and a user got an error 500, what microservice went wrong? with tracing, we can see the request from microservice to microservice to debug the problem.</li> </ul>"},{"location":"Observability/open-telemetry/#so-what","title":"So what?","text":"<p>In order to make a system observable, it must be instrumented. That is, the code must emit\u00a0traces,\u00a0metrics, and\u00a0logs. The instrumented data must then be sent to an Observability back-end. There are a number of Observability back-ends out there, ranging from self-hosted open-source tools to commercial SaaS offerings.</p> <p>In the past, the way in which code was instrumented would vary, as each Observability back-end would have its own instrumentation libraries and agents for emitting data to the tools.</p> <p>This meant that there was no standardized data format for sending data to an Observability back-end. Furthermore, if a company chose to switch Observability back-ends, it meant that they would have to re-instrument their code and configure new agents just to be able to emit telemetry data to the new tool of choice.</p> <p>Recognizing the need for standardization, the cloud community came together, and two open-source projects were born:\u00a0OpenTracing\u00a0(a\u00a0Cloud Native Computing Foundation (CNCF)\u00a0project) and\u00a0OpenCensus\u00a0(a\u00a0Google Open Source\u00a0community project).</p> <p>In the interest of having one single standard, OpenCensus and OpenTracing were merged to form OpenTelemetry (OTel for short)\u00a0in May 2019. As a CNCF incubating project, OpenTelemetry takes the best of both worlds, and then some.</p>"},{"location":"Observability/open-telemetry/#what-is-opentelemetry-otel","title":"What is OpenTelemetry (OTel)","text":"<p>OpenTelemetry, also known as OTel for short, is a vendor-neutral open source Observability framework for instrumenting, generating, collecting, and exporting telemetry data such as\u00a0traces,\u00a0metrics,\u00a0and logs. As an industry standard, it is\u00a0natively supported by a number of vendors.</p> <p>OpenTelemetry is a collection of tools, APIs, and SDKs. Use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help you analyze your software\u2019s performance and behavior.</p> <p>OTel\u2019s goal is to provide a set of standardized vendor-agnostic SDKs, APIs, and\u00a0tools\u00a0for ingesting, transforming, and sending data to an Observability back-end (i.e. open-source or commercial vendor).</p>"},{"location":"Observability/open-telemetry/#what-can-opentelemetry-do-for-me","title":"What can OpenTelemetry do for me?","text":"<p>OTel has broad industry support and adoption from cloud providers,\u00a0vendors\u00a0and end users. It provides you with:</p> <ul> <li>A single, vendor-agnostic instrumentation library\u00a0per language\u00a0with support for both automatic and manual instrumentation.</li> <li>A single vendor-neutral\u00a0collector\u00a0binary that can be deployed in a variety of ways.</li> <li>An end-to-end implementation to generate, emit, collect, process and export telemetry data.</li> <li>Full control of your data with the ability to send data to multiple destinations in parallel through configuration.</li> <li>Open-standard semantic conventions to ensure vendor-agnostic data collection</li> <li>The ability to support multiple\u00a0context propagation\u00a0formats in parallel to assist with migrating as standards evolve.</li> <li>A path forward no matter where you are on your observability journey.</li> </ul> <p>Vendors and Tools with different patterns = Lock-In</p>"},{"location":"Observability/open-telemetry/#what-opentelemetry-is-not","title":"What OpenTelemetry is not","text":"<p>OpenTelemetry is not an observability back-end like Jaeger or Prometheus. Instead, it supports exporting data to a variety of open-source and commercial back-ends. It provides a pluggable architecture so additional technology protocols and formats can be easily added.</p> <p>Refs:</p> <ul> <li>OpenTelemetry</li> </ul>"},{"location":"Observability/service-level/","title":"Service Level","text":""},{"location":"Observability/service-level/#sla","title":"SLA","text":"<p>An SLA (service level agreement) is an agreement between provider and client about measurable metrics like uptime, responsiveness, and responsibilities.</p> <p>These agreements are typically drawn up by a company\u2019s new business and legal teams and they represent the promises you\u2019re making to customers\u2014and the consequences if you fail to live up to those promises. Typically, consequences include financial penalties, service credits, or license extensions.</p> <ul> <li>Agreement you make with your clients or users.</li> <li>Agreement between a vendor and a paying customer.</li> <li>Legal contract that if breached, will have financial penalties.</li> </ul>"},{"location":"Observability/service-level/#slo","title":"SLO","text":"<p>An SLO (service level objective) is an agreement within an SLA about a specific metric like uptime or response time. So, if the SLA is the formal agreement between you and your customer, SLOs are the individual promises you\u2019re making to that customer.</p> <p>SLOs are what set customer expectations and tell IT and DevOps teams what goals they need to hit and measure themselves against.</p> <ul> <li>Intended to define a range of what is most and least acceptable for performance standards;</li> <li>Objectives your team must hit to meet that agreement;</li> <li>99% of requests served in &lt; 400ms over a 28-day window;</li> <li>Disaster recovery time;</li> <li>Application availability;</li> </ul>"},{"location":"Observability/service-level/#sli","title":"SLI","text":"<p>An SLI (service level indicator) measures compliance with an SLO (service level objective). So, for example, if your SLA specifies that your systems will be available 99.95% of the time, your SLO is likely 99.95% uptime and your SLI is the actual measurement of your uptime. Maybe it\u2019s 99.96%. Maybe 99.99%. To stay in compliance with your SLA, the SLI will need to meet or exceed the promises made in that document.</p> <ul> <li>Real numbers on performance</li> </ul> <p>Refs:</p> <ul> <li>Atlassian</li> </ul>"},{"location":"SOLID/","title":"SOLID","text":"<p>SOLID are five principles of object-oriented programming that facilitate software development, making them easy to maintain and extend. These principles can be applied to any OOP language.</p> <p>These principles help the programmer write cleaner code, separating responsibilities, reducing couplings, facilitating refactoring, and encouraging code reuse.</p> <ul> <li>S \u21d2 SRC \u21d2 Single Responsibility Principle</li> <li>O \u21d2 OCP \u21d2 Open-closed Principle</li> <li>L \u21d2 LSP \u21d2 Liskov Substitution Principle</li> <li>I \u21d2 ISP \u21d2 Interface Segregation Principle</li> <li>D \u21d2 DIP \u21d2 Dependency Inversion Principle</li> </ul>"},{"location":"SOLID/dependency-inversion/","title":"Dependency Inversion Principle","text":"<p>Dependency Inversion Principle corresponds to\u00a0D among SOLI\u2019D\u2019 Principles. Its principle starts with this statement.</p> <p>High-level modules should not depend on low-level modules. Both should depend on abstractions.</p> <p>What is Abstraction?</p> <p>Code without abstraction:</p> <pre><code>class Benz  {\n    drive() {\n    }\n\n}\n\nclass CarUtil {\n    drive(benz Benz) {\n        benz.drive();\n    }\n}\n</code></pre> <p>When you change\u00a0<code>drive()</code>\u00a0method inside\u00a0<code>Benz</code>\u00a0class, <code>CarUtil</code> is directly affected. This is prone to make bugs.</p> <p>Tight Coupling is the most undesirable feature in Software</p> <pre><code>interface Car {\n    drive();\n}\n\nclass Benz implements Car {\n    @Override\n    drive() {\n    }\n\n}\n\nclass Tesla implements Car {\n    @Override\n    drive() {\n    }\n\n}\n\nclass CarUtil {\n    drive(car Car) {\n        car.drive();\n    }\n}\n</code></pre> <p>\u201cAbstractions should not depend on details. Details should depend on abstractions.\u201d</p> <pre><code>import java.util.Arrays;\nimport java.util.List;\n\n// High Level Module \nclass ProductCatalog {\n\n    public void listAllProducts() {\n\n        // High Level Module depends on Abstraction \n        ProductRepository productRepository =  new SQLProductRepository();\n\n        List&lt;String&gt; allProductNames = productRepository.getAllProductNames();\n\n        // Display product names\n    }\n\n}\n\ninterface ProductRepository {\n    List&lt;String&gt; getAllProductNames();\n\n}\n\n// Low Level Module\nclass SQLProductRepository implements ProductRepository {\n\n    public List&lt;String&gt; getAllProductNames() {\n        return Arrays.asList(\"soap\", \"toothpaste\", \"shampoo\");\n    }\n\n}\n</code></pre> <p>Why doing this?</p> <p>First, you don\u2019t know what database you are going to use. It may not be specifically\u00a0<code>SQL</code>.</p> <p>Second, <code>ProductCatalog</code>\u2018s\u00a0<code>listAllProducts()</code>\u00a0does not depend on a specific object. This means, when you change code in\u00a0<code>SQLProductRepository</code>,\u00a0<code>ProductCatalog</code>\u00a0is not directly affected. You just have achieved\u00a0<code>loose-coupling</code>.</p>"},{"location":"SOLID/interface-segregation/","title":"Interface Segregation Principle","text":"<p>No client should be forced to depend on methods it does not use</p> <p>\u201cYOU SHOULD ONLY DEFINE METHODS THAT ARE GOING TO BE USED\u201d.</p> <p>This principle basically says that it's better to create more specific interfaces rather than having a single generic interface.</p> <p>Wrong:</p> <pre><code>interface IMultiFunction {\n    public void print();\n    public void getPrintSpoolDetails();\n    public void scan();\n    public void scanPhoto();\n    public void fax();\n    public void internetFax();\n}\n\nclass CanonPrinter implements IMultiFunction {\n\n    @Override\n    public void print() {}\n\n    @Override\n    public void getPrintSpoolDetails() {}\n\n    /* This machine can't scan */\n    @Override\n    public void scan() {}\n\n    /* This machine can't scan photo */\n    @Override\n    public void scanPhoto() {}\n\n    /* This machine can't fax */\n    @Override\n    public void fax() {}\n\n     /* This machine can't fax on internet */\n    @Override\n    public void internetFax() {}\n\n}\n</code></pre> <p>A class should not be required to implement interfaces that it will not use.</p> <p>Correct:</p> <pre><code>interface Movie {\n    public function play();\n}\ninterface AudioControl {\n    public function increaseVolume();\n}\n\nclass TheLionKing implements Movie, AudioControl {\n    public function play() {}\n    public function increaseVolume() {}\n}\n\nclass ModernTimes implements Movie {\n    public function play() {}\n}\n</code></pre>"},{"location":"SOLID/liskov-substitution/","title":"Liskov Substitution Principle","text":"<p>Objects should be replaceable with their subtypes without affecting the correctness of the program</p> <p>Subclasses can be substituted for their parent classes.</p> <p>Class Y extending class X, class X must be replaceable by class Y.</p> <p>Example of the duck, a real duck and a rubber duck, the battery duck does the same thing as the real duck, however, it uses batteries, so it is violating this principle because it is not a real duck.</p> <p>If S is a subtype of T, then objects of type T, in a program, can be replaced by objects of type S without changing the program's properties.</p> <p>Wrong:</p> <pre><code>class Car {\n    double getCabinWidth() {\n    // return cabin width \n    }   \n}\n\nclass RacingCar extends Car {\n\n    public double getCabinWidth() {\n        // ...\n    }\n\n    public double getCockpitWidth() {\n        // return cockpit width \n    }\n}\n</code></pre> <p>the Car interface cannot be replaced by RacingCar so it is hurting this principle.</p>"},{"location":"SOLID/open-closed/","title":"Open-closed Principle","text":"<p>Software components should be closed for modification, but open for extension.</p> <p>that is, when new behaviors and features need to be added to the software, we should extend and not change the original source code.</p> <p>Example:</p> <pre><code>class InsuranceCompany {\n    discountRate(customer VehicleInsuranceCustomer) {\n        if(customer.isLoyalCustomer()) {\n            return 20;\n        }\n        return 10;\n    }\n}\n\nclass VehicleInsuranceCustomer {\n    public boolean isLoyalCustomer() {\n        return true; \n    }\n}\n</code></pre> <p><code>InsuranceCompany</code> has the\u00a0<code>discountRate()</code> method. But as you can see, the parameter of this method only accepts the <code>VehichleInsuranceCustomer</code> type. But what if we want to introduce another type of customer? we would have to add another method.</p> <p>Ex:</p> <pre><code>class InsuranceCompany {\n\n    discountRate(customer VehicleInsuranceCustomer) {\n        if(customer.isLoyalCustomer()) {\n            return 20; \n        }        \n        return 10;\n    }\n\n    discountRate(customer HomeInsuranceCustomer) {\n        if(customer.isLoyalCustomer()) {\n            return 20; \n        }        \n        return 10;\n    }\n\n    discountRate(customer LifeInsuranceCustomer) {\n      if(customer.isLoyalCustomer()) {\n          return 20; \n      }        \n        return 10;\n    }\n\n}\n\nclass VehicleInsuranceCustomer {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass HomeInsuranceCustomer {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass LifeInsuranceCustomer {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n</code></pre> <p>This is the problem. You have to keep introducing duplicate code because the method is not \u201copen for extension\u201d, it does not allow dynamic input. This makes the code difficult to maintain and fix.</p> <p>Solution: Work with abstractions</p> <pre><code>class InsuranceCompany {\n    // Parameter is now abstracted \n    discountRate(customer CustomerProfile) {\n        if(customer.isLoyalCustomer()) {\n            return 20; \n        }        \n        return 10;\n    }\n\n}\n\n// Core \ninterface CustomerProfile {\n    isLoyalCustomer(): boolean;\n}\n\nclass VehicleInsuranceCustomer implements CustomerProfile {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass HomeInsuranceCustomer implements CustomerProfile {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n\nclass LifeInsuranceCustomer implements CustomerProfile {\n    isLoyalCustomer() {\n        return true; \n    }\n}\n</code></pre> <p>Classes that need to communicate with InsuranceCompany must implement an interface, so we're making an abstraction. By doing this we can add more customers (open for extension), and also we are not obliged to modify the InsuranceCompany code (closed for modification).</p> <p>Benefits:</p> <ul> <li>Easy to add new features;</li> <li>Less development and testing cost.</li> <li>Open Closed Principle requires decoupling, which ends up following the Single Responsibility Principle.</li> </ul>"},{"location":"SOLID/single-responsability/","title":"Single Responsibility Principle","text":"<p>A class should have one, and only one, reason to change.</p> <p>This principle states that a class must be specialized in a single subject and have only one responsibility within the software, that is, the class must have a single task or action to perform.</p> <p>A class should have only one responsibility, it cannot do more than it was created to do.</p> <ul> <li>\"Every Software Component should have only one responsibility.\"</li> </ul> <p>A software component can be understood as a class, function, method, or even a module.</p> <p>It's very common when we are developing a software to give a class more than one responsibility and end up creating classes that do it all \u2014 God Class*. At first, this may seem efficient, but as the responsibilities end up getting mixed up, when there is a need to make changes to this class, it will be difficult to modify one of these responsibilities without compromising the others. Every change ends up being introduced with a certain level of uncertainty in our system \u2014 especially if there are no automated tests!</p> <p>*God Class: In object-oriented programming, a class that knows too much or does too much.</p> <p>Bad code:</p> <pre><code>/** Before Refactoring **/  \n// Too Many Responsibilities \n// - Measurements of squares \n// - Rendering images of squares \nclass Square {\n\n    int side = 5;\n\n    public int calculateArea() {\n        return side * side;\n    }\n\n    public int calculatePerimeter() {\n        return side * 4; \n    }\n\n    public void draw() {\n        if (highResolutionMonitor) {\n            // Render a high resolution image of a square\n        } else {\n            // Render a normal image of a square\n        }\n    }\n\n    public void rotate(int degree) {\n        // Rotate the image of the square clockwise to\n        // the required degree and re-render \n    }\n\n}\n</code></pre> <p>The Square class has two responsibilities, measuring the area of the square and rendering the image to the screen.</p> <p>Refactoring:</p> <pre><code>/** After Refactoring **/  \n// Responsibility: Measurements of squares \n// After Refactoring\nclass Square {\n\n    int side = 5;\n\n    public int calculateArea() {\n        return side * side; \n    }\n\n    public int calculatePerimeter() {\n        return side * 4; \n    }\n\n}\n\n// Responsibility: Rendering images of squares \nclass SquareUI {\n\n    public void draw() {\n        if (highResolutionMonitor) {\n            // Render a high resolution image of a square\n        } else {\n            // Render a normal image of a square\n        }\n    }\n\n    public void rotate(int degree) {\n        // Rotate the iamge of the square clockwise to\n        // the required degree and re-render \n    }\n\n}\n</code></pre> <p>Now Square calculates the area and SquareUI handles the rendering.</p> <ul> <li><code>calculateArea()</code>\u00a0and\u00a0<code>calculatePerimeter()</code>\u00a0are related to the square calculation;</li> <li><code>draw()</code>\u00a0and <code>rotate()</code>\u00a0is related to rendering;</li> </ul> <p>Example 2:</p> <pre><code>// Responsibility 1: Handle core student profile data\n// Responsibility 2: Handle Database Operations \nclass Student {\n\n    private String studentId;\n    private String address;\n\n    public void save() {\n        // Serialize object into a string representation \n        String objectStr = MyUtils.serialzieIntoAString(this);\n        Connection connection = null; \n        Statement stmt = null; \n\n        // We are using MYSQL\n        // What if I want to use another database?\n        // This is why Tight Coupling is bad practices for prgramming \n        try {\n            Class.forName(\"com.mysql.jdbc.Driver\");\n            connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/MyDb\", \"root\", \"password\");\n            stmt = connection.createStatement();\n            stmt.execute(\"INSERT INTO STUDENTS VALUES (\" + objectStr + \")\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n    }\n\n    public String getStudentId() {\n        return studentId;\n    }\n\n    public void setStudentId(String studentId) {\n        this.studentId = studentId;\n    }\n\n}\n</code></pre> <p>Refactoring:</p> <pre><code>/** After Refactoring **/ \n// Responsibility: Handle Database Operations  \npublic class StudentRepository {\n    public void save(student Student) {\n        // We are using MYSQL\n        // What if I want to use another database?\n        // This is why Tight Coupling is bad practices for prgramming \n        try {\n            Class.forName(\"com.mysql.jdbc.Driver\");\n            connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/MyDb\", \"root\", \"password\");\n            stmt = connection.createStatement();\n            stmt.execute(\"INSERT INTO STUDENTS VALUES (\" + student + \")\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n    }\n}\n\n// Responsibility: Handle core student profile data\nclass Student {\n\n    private String studentId;\n    private Date studentDOB;\n    private String address;\n\n    public String getStudentId() {\n        return studentId;\n    }\n\n    public void setStudentId(String studentId) {\n        this.studentId = studentId;\n    }\n\n}\n\nstudent = new Student(...)\nstudentRepository = StudentRepository(...)\n\nstudentRepository.save(student)\n</code></pre>"},{"location":"SystemsCommunication/","title":"Systems Communication","text":"<p>Communication between different components or systems is often necessary. Synchronous communication is a type of communication in which a request is sent and the system waits for a response before continuing. This type of communication is used when the system requires an immediate response to continue its processing.</p> <p>On the other hand, asynchronous communication is a type of communication in which a request is sent and the system doesn't require an immediate response. The system can continue its processing without waiting for a response. The response will be processed later when it's available.</p> <p>Synchronous communication \u21d2 sends a request and waits for a response. Examples: phone call, question and answer.</p> <p>Asynchronous communication \u21d2 sends a request and doesn't require an immediate response. Examples: postal service, sends a letter and doesn't expect a response at that moment.</p> <p>In software development, when building a web application, synchronous communication is used between the client and the server. When a user submits a request, the server processes the request and sends back a response immediately. However, asynchronous communication is used when performing background tasks such as sending an email or processing a large file. The user doesn't need to wait for the task to complete before continuing to use the application.</p>"},{"location":"SystemsCommunication/#some-types-of-system-communication","title":"Some Types of System Communication","text":"<ul> <li>REST</li> <li>gRPC</li> <li>GraphQL</li> </ul>"},{"location":"SystemsCommunication/GraphQL/","title":"GraphQL","text":"<p>Query language for APIs that allows you to request and receive only the data you need, in a single request, reducing the number of API requests required. It was developed by Facebook in 2012 and was publicly released in 2015.</p> <ul> <li>Hierarchical: Queries are hierarchical, meaning that the shape of the query matches the shape of the data returned, allowing for more efficient and flexible data retrieval.</li> <li>Strongly Typed: Provides a type system that allows you to specify the structure of your data and catch errors before they happen.</li> <li>Client-Specified Queries: The client specifies the data it needs, allowing for more efficient API requests and reduced bandwidth usage.</li> <li>Schema-Driven: The schema is the contract between the server and the client, defining the data types, fields, and operations that can be performed.</li> </ul> <p>\"Queries\" perform queries and retrieve data according to the request. They are executed in parallel.</p> <p>\"Mutation\" performs the process of create, update, and delete. It is executed serially.</p> <p>In which cases can we use it?</p> <ul> <li>When you need flexible and efficient API for querying data.</li> <li>When you have complex or nested data structures.</li> <li>When you want to reduce the number of API requests required.</li> </ul> GraphQL REST Allow to retrieve data from multiple resources in a single request. Clients must make separate requests for each resource they need. Clients retrieve only the data they need. Clients receive all the data associated with a resource, whether or not they need it. Provides documentation and validation for clients. Does not have a standardized schema, which can make it difficult for clients to understand the structure and relationships of resources."},{"location":"SystemsCommunication/REST/","title":"REST","text":"<p>If you are building REST APIs or REST Services you're using HTTP. Technically, REST services can be provided over any application layer protocol as long as they conform to certain properties. In practice, basically, everyone uses HTTP Protocol.</p> <p>Advantages:</p> <ul> <li>Simplicity</li> <li>Stateless \u2192 The server doesn't maintain any information about the client. Each request contains all the information necessary for the server to process it. Each request is a new request, which is why we need to pass headers all the time (JWT Token, etc.).</li> <li>Cacheable</li> </ul>"},{"location":"SystemsCommunication/REST/#rest-vs-restful","title":"REST vs RESTful","text":"<p>A REST API (also known as RESTful API) is an application programming interface (API or web API) that conforms to the constraints of REST architectural style and allows for interaction with RESTful web services.</p> <p>An API is a set of rules that define how applications can connect to and communicate with each other. A REST API is an API that conforms to the design principles of the REST. For this reason, REST APIs are sometimes referred to RESTful APIs.</p> <p>The only requirement for an API implement REST is that they align to the following six REST design principles - also known as architectural constraints:</p> <ol> <li> <p>Uniform interface. It indicates that the server transfers information in a standard format. All API requests for the same resource should look the same, no matter where the request comes from. The REST API should ensure that the same piece of data, such as the name or email address of a user, belongs to only one uniform resource identifier (URI). Resources shouldn\u2019t be too large but should contain every piece of information that the client might need. Uniform interface imposes four architectural constraints:</p> <ul> <li>Requests should identify resources. They do so by using a uniform resource identifier.</li> <li>Clients have enough information in the resource representation to modify or delete the resource if they want to. The server meets this condition by sending metadata that describes the resource further.</li> <li>Clients receive information about how to process the representation further. The server achieves this by sending self-descriptive messages that contain metadata about how the client can best use them.</li> <li>Clients receive information about all other related resources they need to complete a task. The server achieves this by sending hyperlinks in the representation so that clients can dynamically discover more resources.</li> </ul> </li> <li> <p>Client-server decoupling. In REST\u00a0API design, client and server applications must be completely independent of each other. The only information the client application should know is the URI of the requested resource; it can't interact with the server application in any other ways. Similarly, a server application shouldn't modify the client application other than passing it to the requested data via HTTP.</p> </li> <li> <p>Statelessness. statelessness refers to a communication method in which the server completes every client request independently of all previous requests. Clients can request resources in any order, and every request is stateless or isolated from other requests. This REST API design constraint implies that the server can completely understand and fulfill the request every time. REST APIs are stateless, meaning that each request needs to include all the information necessary for processing it. In other words, REST APIs do not require any server-side sessions. Server applications aren\u2019t allowed to store any data related to a client request. No client information is stored between get requests and each request is separate and unconnected.</p> </li> <li> <p>Cacheability. When possible, resources should be cacheable on the client or server side. Server responses also need to contain information about whether caching is allowed for the delivered resource. The goal is to improve performance on the client side, while increasing scalability on the server side.</p> </li> <li> <p>Layered system architecture. In REST APIs, the calls and responses go through different layers. As a rule of thumb, don\u2019t assume that the client and server applications connect directly to each other. There may be a number of different intermediaries in the communication loop. REST APIs need to be designed so that neither the client nor the server can tell whether it communicates with the end application or an intermediary. In a layered system architecture, the client can connect to other authorized intermediaries between the client and server, and it will still receive responses from the server. Servers can also pass on requests to other servers. You can design your RESTful web service to run on several servers with multiple layers such as security, application, and business logic, working together to fulfill client requests. These layers remain invisible to the client.</p> </li> <li> <p>Code on demand (optional). REST APIs usually send static resources, but in certain cases, responses can also contain executable code (such as Java applets). In these cases, the code should only run on-demand. In REST architectural style, servers can temporarily extend or customize client functionality by transferring software programming code to the client. For example, when you fill a registration form on any website, your browser immediately highlights any mistakes you make, such as incorrect phone numbers. It can do this because of the code sent by the server.</p> </li> </ol>"},{"location":"SystemsCommunication/REST/#maturity-levels","title":"Maturity levels","text":"<p>Level 0: The Swamp of POX</p> <p>Performing an operation on the server without any standardization.</p> <p>The Swamp of POX (Plain Old XML) means that you\u2019re using HTTP. Technically, REST services can be provided over any application layer protocol as long as they conform to certain properties. In practice, basically, everyone uses HTTP.</p> <p>Level zero of maturity does not make use of any of URI, HTTP Methods, and HATEOAS capabilities.</p> <p>Level 1: Use of resources</p> <p>REST\u2019s \u2018resources\u2019 are the core pieces of data that your application acts on. These will often correspond to the Models in your application</p> <p>API design at Level 1 is all about using different URLs to interact with the different resources in your application.</p> URL Operation /products/1 Retrieve /products Insert /products/1 Update /products/1 Remove <p>Level 2: HTTP Verbs</p> <p>If we want to get a list of Pages, we make a GET request to <code>/page</code>, but if we want to create a new Page, we use POST rather than GET to the same resource - <code>/page</code>.</p> Verb Scope Semantics GET collection Retrieve all resources in a collection GET resource Retrieve a single resource HEAD collection Retrieve all resources in a collection (header only) HEAD resource Retrieve a single resource (header only) POST collection Create a new resource in a collection PUT resource Update a resource PATCH resource Update a resource DELETE resource Delete a resource OPTIONS any Return available HTTP methods and other options <p>The use of HTTP verbs like GET, POST, PUT, and DELETE is another important principle of REST. These verbs are used to describe the operation being performed on a resource. For example, GET is used to retrieve information, POST is used to create a new resource, PUT is used to update an existing resource, and DELETE is used to remove a resource.</p> <p>Level 3: HATEOAS: Hypermedia as the Engine of Application State</p> <p>Responds to the endpoint by returning what else can be done based on what was just done.</p> <pre><code>{\n    \"account\": {\n        \"account_number\": 123,\n        \"balance\" : {\n            \"currency\": \"usd\",\n            \"value\": 100.00\n        },\n        \"links\": {\n            \"deposit\": \"/accounts/123/deposit\",\n            \"withdraw\": \"/accounts/123/withdraw\",\n            \"transfer\": \"/accounts/123/transfer\",\n            \"close\": \"/accounts/123/close\"\n        }   \n    }\n\n}\n</code></pre> <p>HATEOAS is an advanced concept in REST that allows a server to provide information to the client about what actions can be performed next. This allows for a more dynamic and flexible API design.</p>"},{"location":"SystemsCommunication/gRPC/","title":"gRPC","text":"<p>gRPC is a high-performance framework developed by Google that facilitates communication between systems in an extremely fast and lightweight way, independent of language. gRPC utilizes Remote Procedure Call (RPC) to allow the client to call a function on the server, enabling developers to build scalable and distributed systems with ease.</p> <p>One of the main benefits of gRPC is its speed. By utilizing Protocol Buffers, a mechanism created and used by Google to serialize structured data, gRPC can send and receive data faster than traditional text-based formats like JSON. Additionally, gRPC supports bidirectional streaming using HTTP/2, which allows data to be sent in binary format and compressed, reducing network resource consumption and latency.</p> <p>gRPC is ideal for microservices architectures and can be used in a variety of environments, including mobile, browsers, and backend applications. It also offers automatic library generation, which can significantly reduce development time.</p> <p>In which cases can we use it?</p> <ul> <li>Ideal for microservices;</li> <li>Mobile, Browsers, and Backend;</li> <li>Automatic library generation;</li> <li>Bidirectional streaming using HTTP/2 (Faster than HTTP 1, allows sending data in binary format, data streaming, etc.)</li> </ul> <p>Languages with official support:</p> <ul> <li>gRPC-GO</li> <li>gRPC-JAVA</li> <li>gRPC-C<ul> <li>C++</li> <li>Python</li> <li>Ruby</li> <li>Objective C</li> <li>PHP</li> <li>C#</li> <li>Node.js</li> <li>Dart</li> <li>Kotlin / JVM</li> </ul> </li> </ul> <p>RPC \u21d2 Remote Procedure Call</p> <p>The client calls the function on the server</p> <pre><code>    Client                          Server\nserver.soma(a,b)                 func soma(int a, int b) {}\n</code></pre> <p>Protocol Buffers \u21d2 is a mechanism created and used by Google to serialize structured data. It defines how you want the data to be structured - in a file with the extension <code>.proto</code>.</p> <pre><code>syntax = \"proto3\"\n\nmessage SearchRequest {\n    string query = 1;\n    int32 page_number = 2;\n    int32 result_per_page = 3;\n}\n</code></pre> <p>Protocol Buffers vs JSON While JSON is a popular format for transmitting data, it has some drawbacks in terms of performance and network resource consumption. Protocol Buffers, on the other hand, offer several advantages over JSON:</p> <ul> <li>Binary files &lt; JSON, JSON is heavier because it is plain text;</li> <li>Lightweight: The process of serialization and deserialization with Protocol Buffers is more lightweight than with JSON, requiring less CPU resources.</li> <li>Network resource consumption: Protocol Buffers consume less network resources than JSON.</li> <li>Process is faster;</li> </ul> <p>HTTP/2</p> <ul> <li>Launched in 2015;</li> <li>The transmitted data is binary and not text like in HTTP 1.1;</li> <li>Uses the same TCP connection to send and receive data from the client and server (Multiplex);</li> <li>Server push;</li> <li>Headers are compressed;</li> <li>Uses fewer network resources;</li> <li>Process is faster.</li> </ul> <p>REST vs gRPC</p> REST gRPC Text / JSON Protocol Buffers Unidirecional Bidirecional e Asynchronous High Latency Low Latency No contract (higher chance of errors) Defined contract (.proto) No streaming support (Request / Response) Streaming support"}]}